{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, paired_euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import fasttext\n",
    "import sister\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/')\n",
    "from utils import *\n",
    "\n",
    "import spacy\n",
    "\n",
    "# ft = fasttext.load_model('/home/roshansk/YelpAnalysis/RandomForest/cc.en.300.bin')\n",
    "\n",
    "# embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')\n",
    "# nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl_subword/')\n",
    "\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/Datasets/')\n",
    "from createDataset import *\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVM(trainData, testData, trainDf, testDf):\n",
    "  \n",
    "    model = LinearSVC(random_state=1, dual=False, max_iter=10000) \n",
    "    model.fit(trainData, trainDf.label)  \n",
    "    pred = model.predict(testData)\n",
    "         \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def generateLexicon(NNnet, trainDf, nlp):\n",
    "    return getWordPred_SVM(model, trainDf, nlp)\n",
    "\n",
    "def testSVM_direct( model, dataset, lexiconWords, lexiconMap, nlp,  train = False):\n",
    "    \n",
    "    if dataset != 'dialog':\n",
    "        trainDf, devDf, testDf = getData(dataFolder, dataset)\n",
    "    else:\n",
    "        testDf = getExternalData(dataFolder, 'dialog')\n",
    "\n",
    "        \n",
    "    if train:\n",
    "        testData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "        testDf = trainDf\n",
    "    else:\n",
    "        testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "        \n",
    "    \n",
    "    pred = model.predict(testData)\n",
    "    \n",
    "    modelAcc = np.round(accuracy_score(testDf.label, pred),3)\n",
    "    modelF1 = np.round(f1_score(testDf.label, pred),3)\n",
    "    \n",
    "#     print('Model Evaluation')\n",
    "#     print(f\"Acc : {acc} F1 : {f1}\" )\n",
    "    \n",
    "#     print(\"Lexicon Evalution\")\n",
    "    lexAcc, lexF1 = evaluateLexicon(testDf, lexiconWords, lexiconMap, nlp, returnScores = True)\n",
    "    \n",
    "    print(f\"{dataset} , {modelAcc} , {modelF1} , {lexAcc} , {lexF1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf, devDf, testDf = getData(dataFolder, 'nrc_joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "\n",
    "testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainSVM(trainData, testData, trainDf, testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = generateLexicon(model,trainDf, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>SVM_Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>joy-</td>\n",
       "      <td>13.591259</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18074</th>\n",
       "      <td>lemsip</td>\n",
       "      <td>11.103914</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>carols</td>\n",
       "      <td>10.273476</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24256</th>\n",
       "      <td>unltd</td>\n",
       "      <td>9.931378</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10966</th>\n",
       "      <td>m25</td>\n",
       "      <td>9.796602</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word      score  wordCount  SVM_Rank\n",
       "7478     joy-  13.591259          2         0\n",
       "18074  lemsip  11.103914          2         1\n",
       "6969   carols  10.273476         13         2\n",
       "24256   unltd   9.931378          1         3\n",
       "10966     m25   9.796602          3         4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconWords, lexiconMap = getLexicon(0, df = lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13538/13538 [00:05<00:00, 2636.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog , 0.739 , 0.82 , 0.838 , 0.907\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'dialog',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1242.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_finefood_subset , 0.756 , 0.839 , 0.874 , 0.929\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'amazon_finefood_subset',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1189.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_subset , 0.879 , 0.917 , 0.888 , 0.927\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'yelp_subset',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12452/12452 [00:10<00:00, 1189.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_subset , 0.908 , 0.907 , 0.909 , 0.908\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'yelp_subset',lexiconWords, lexiconMap, nlp, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [00:00<00:00, 2468.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrc_joy , 0.589 , 0.531 , 0.628 , 0.369\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'nrc_joy',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [00:00<00:00, 1410.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empathy , 0.516 , 0.118 , 0.532 , 0.592\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'empathy',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 958.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_toys_subset , 0.867 , 0.926 , 0.935 , 0.966\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'amazon_toys_subset',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60579/60579 [00:53<00:00, 1135.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_1v5 , 0.952 , 0.967 , 0.958 , 0.972\n"
     ]
    }
   ],
   "source": [
    "testSVM_direct(model, 'yelp_1v5',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog , 0.556 , 0.647 , 0.819 , 0.899\n",
      "amazon_finefood_subset , 0.398 , 0.468 , 0.851 , 0.919\n",
      "amazon_toys_subset , 0.354 , 0.48 , 0.94 , 0.969\n",
      "yelp_subset , 0.497 , 0.52 , 0.778 , 0.869\n",
      "empathy , 0.484 , 0.02 , 0.532 , 0.603\n",
      "nrc_joy , 0.747 , 0.704 , 0.752 , 0.676\n"
     ]
    }
   ],
   "source": [
    "dataList = ['dialog','amazon_finefood_subset','amazon_toys_subset','yelp_subset','empathy','nrc_joy']\n",
    "for data in dataList:\n",
    "    testSVM_direct(model, data,lexiconWords, lexiconMap, nlp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
