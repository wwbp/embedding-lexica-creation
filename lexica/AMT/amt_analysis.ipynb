{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infectious-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "young-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_filter(df, attention_checker):\n",
    "    not_use = 0\n",
    "    index = []\n",
    "    for i in range(len(df)):\n",
    "        sub = df.iloc[i]\n",
    "        not_pass = 0\n",
    "        for word in range(1,61):\n",
    "            input_col = 'Input.word'+str(word)\n",
    "            word_col = 'Answer.word'+str(word)+'-radios'\n",
    "            if sub[input_col] in attention_checker:\n",
    "                if sub[word_col] != attention_checker[sub[input_col]]:\n",
    "                    not_pass += 1\n",
    "        if not_pass > 2:\n",
    "            index.append(i)\n",
    "            not_use += 1\n",
    "\n",
    "    print(df.shape[0])\n",
    "    print(not_use)\n",
    "    #notuse.append(not_use)\n",
    "\n",
    "    if not_use == 0:\n",
    "        return index, df\n",
    "    \n",
    "    if not_use == 1:\n",
    "        df_valid = pd.concat([df.iloc[:index[0]],df.iloc[index[0]+1:]])\n",
    "        return index, df_valid\n",
    "    \n",
    "    df_valid = []\n",
    "    for i in range(len(index)):\n",
    "        if i != len(index)-1:\n",
    "            df_valid.append(df.iloc[index[i]+1:index[i+1]])\n",
    "        else:\n",
    "            if index[i] != len(df) - 1:\n",
    "                df_valid.append(df.iloc[index[i]+1:])\n",
    "\n",
    "    df_valid = pd.concat(df_valid)\n",
    "    \n",
    "    return index, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "regulation-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_stat(df, df_valid, index, task):\n",
    "    worker_mean_kappa = {}\n",
    "    for worker in df[\"WorkerId\"].unique():\n",
    "        workersub = df[df[\"WorkerId\"] == worker]\n",
    "        if worker not in worker_mean_kappa:\n",
    "            worker_mean_kappa[worker] = [len(workersub)]\n",
    "        hits = workersub.HITId.unique()\n",
    "        for hit in hits:\n",
    "            workerhit = workersub[workersub[\"HITId\"] == hit]\n",
    "            hitsub = df_valid[df_valid['HITId'] == hit]\n",
    "            if len(hitsub) <= 1:\n",
    "                continue\n",
    "            word_col = []\n",
    "            for word in range(1,61):\n",
    "                word_col.append('Answer.word'+str(word)+'-radios')\n",
    "            for i in range(len(hitsub)):\n",
    "                if hitsub.iloc[i][\"WorkerId\"] != worker:\n",
    "                    score = cohen_kappa_score(hitsub.iloc[i][word_col].astype(int), workerhit.iloc[0][word_col].astype(int), labels=[1,2,3])\n",
    "                    if np.isnan(score):\n",
    "                        score = 1\n",
    "                    worker_mean_kappa[worker].append(score)\n",
    "\n",
    "    worker_list = {}\n",
    "    for worker in worker_mean_kappa:\n",
    "        if worker not in worker_list:\n",
    "            if len(worker_mean_kappa[worker]) == 1:\n",
    "                worker_list[worker] = [worker_mean_kappa[worker][0], 0, np.nan, 0, 0]\n",
    "            else:\n",
    "                mean = round(np.mean(worker_mean_kappa[worker][1:]), 3)\n",
    "                bad_times = sum([i<0.3 for i in worker_mean_kappa[worker][1:]])\n",
    "                good_times = sum([i>0.55 for i in worker_mean_kappa[worker][1:]])\n",
    "#                 print(worker, worker_mean_kappa[worker][1:], mean, bad_times, good_times)\n",
    "                worker_list[worker] = [worker_mean_kappa[worker][0], 0, mean, bad_times, good_times]\n",
    "\n",
    "    temp = {}\n",
    "    for worker in df.iloc[index].WorkerId:\n",
    "        if worker in temp:\n",
    "            temp[worker] += 1\n",
    "        else:\n",
    "            temp[worker] = 1\n",
    "\n",
    "    for worker in temp:\n",
    "        if temp[worker]/worker_list[worker][0]>0.3:\n",
    "            worker_list[worker][1] = 1\n",
    "\n",
    "    worker_data = pd.DataFrame(worker_list).T\n",
    "    worker_data.columns = [task+i for i in [\" HITs Count\", \" No Pass\", \" Mean Kappa\", \" Bad Times\", \" Good Times\"]]\n",
    "    \n",
    "    return worker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "indoor-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_ck(df_valid):\n",
    "    ck = []\n",
    "    for hit in df_valid['HITId'].unique():\n",
    "        sub = df_valid[df_valid['HITId'] == hit]\n",
    "        if len(sub) <= 1:\n",
    "            continue\n",
    "        word_col = []\n",
    "        for word in range(1,61):\n",
    "            word_col.append('Answer.word'+str(word)+'-radios')\n",
    "        for i in range(len(sub)-1):\n",
    "            for j in range(i+1, len(sub)):\n",
    "                score = cohen_kappa_score(sub.iloc[i][word_col].astype(int), sub.iloc[j][word_col].astype(int), labels=[1,2,3])\n",
    "                if np.isnan(score):\n",
    "                    score = 1\n",
    "                ck.append(score)\n",
    "\n",
    "    print('Average CK:{:.3f}'.format(np.mean(ck)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "european-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_classification(df_valid, threshold=0.8):\n",
    "    word2score = {}\n",
    "    for answer_ind in range(len(df_valid)):\n",
    "        answer = df_valid.iloc[answer_ind]\n",
    "        for word in range(1,61):\n",
    "            input_col = 'Input.word'+str(word)\n",
    "            word_col = 'Answer.word'+str(word)+'-radios'\n",
    "            #score = max(3 - answer[word_col].astype(int), 0)\n",
    "            score = 1 if answer[word_col].astype(int) < 3 else 0\n",
    "            if answer[input_col] not in word2score:\n",
    "                word2score[answer[input_col]] = [score, 1]\n",
    "            else:\n",
    "                word2score[answer[input_col]][0] = word2score[answer[input_col]][0]+score\n",
    "                word2score[answer[input_col]][1] = word2score[answer[input_col]][1]+1\n",
    "                \n",
    "    desB = []\n",
    "    # relB = []\n",
    "    no = []\n",
    "\n",
    "    for word in word2score:\n",
    "        average = word2score[word][0]/word2score[word][1]\n",
    "        #if average > 1.6:\n",
    "        if average >= threshold:\n",
    "            desB.append(word)\n",
    "        #elif average < 0.6:\n",
    "        else:\n",
    "            no.append(word)\n",
    "        #else:\n",
    "        #    relB.append(word)\n",
    "                \n",
    "    return desB, no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "synthetic-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(desB, no, positivedata, filename, total):\n",
    "    stat = {}\n",
    "    for method in alldata.Method.unique():\n",
    "        stat[method] = {\"right\": {\"one\":[], \"freq\":[]}, \"wrong\":{\"one\":[], \"freq\":[]}}\n",
    "\n",
    "    for word in desB:\n",
    "        sub = positivedata[positivedata[\"Word\"] == word]\n",
    "        for ind, method in enumerate(sub[\"Method\"]):\n",
    "            if sub.iloc[ind][\"Freq\"] == \"one\":\n",
    "                stat[method][\"right\"][\"one\"].append(word)\n",
    "            else:\n",
    "                stat[method][\"right\"][\"freq\"].append(word)\n",
    "\n",
    "    for word in no:\n",
    "        sub = positivedata[positivedata[\"Word\"] == word]\n",
    "        for ind, method in enumerate(sub[\"Method\"]):\n",
    "            if sub.iloc[ind][\"Freq\"] == \"one\":\n",
    "                stat[method][\"wrong\"][\"one\"].append(word)\n",
    "            else:\n",
    "                stat[method][\"wrong\"][\"freq\"].append(word)\n",
    "\n",
    "    for method in stat:\n",
    "        print(method, len(stat[method]['right']['one'])/total, len(stat[method]['right']['freq'])/total)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(stat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f60d2e42-bf75-4944-93c0-ce3452867e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Roberta_Mask', 'Roberta_Partition', 'DistilBERT_Mask',\n",
       "       'DistilBERT_Partition', 'FFN', 'SVM', 'LSTM', 'Univariant'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.Method.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22577219-0547-4204-985f-5b6a61c225f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result_new(desB, no, alldata, filename, total):\n",
    "    with open(filename, 'r') as f:\n",
    "        former = json.load(f)\n",
    "\n",
    "    r_1_true = []\n",
    "    r_m_true = []\n",
    "    r_1_wrong = []\n",
    "    r_m_wrong = []\n",
    "    d_1_true = []\n",
    "    d_m_true = []\n",
    "    d_1_wrong = []\n",
    "    d_m_wrong = []\n",
    "    for i in former['Roberta_Partition']['right']['one']:\n",
    "        if i in list(alldata[(alldata['Method']=='Roberta_Partition') & (alldata['Freq']=='one')].Word):\n",
    "            r_1_true.append(i)\n",
    "    for i in former['Roberta_Partition']['right']['freq']:\n",
    "        if i in list(alldata[(alldata['Method']=='Roberta_Partition') & (alldata['Freq']=='multi')].Word):\n",
    "            r_m_true.append(i)\n",
    "    for i in former['Roberta_Partition']['wrong']['one']:\n",
    "        if i in list(alldata[(alldata['Method']=='Roberta_Partition') & (alldata['Freq']=='one')].Word):\n",
    "            r_1_wrong.append(i)\n",
    "    for i in former['Roberta_Partition']['wrong']['freq']:\n",
    "        if i in list(alldata[(alldata['Method']=='Roberta_Partition') & (alldata['Freq']=='multi')].Word):\n",
    "            r_m_wrong.append(i)\n",
    "    for i in former['DistilBERT_Partition']['right']['one']:\n",
    "        if i in list(alldata[(alldata['Method']=='DistilBERT_Partition') & (alldata['Freq']=='one')].Word):\n",
    "            d_1_true.append(i)\n",
    "    for i in former['DistilBERT_Partition']['right']['freq']:\n",
    "        if i in list(alldata[(alldata['Method']=='DistilBERT_Partition') & (alldata['Freq']=='multi')].Word):\n",
    "            d_m_true.append(i)\n",
    "    for i in former['DistilBERT_Partition']['wrong']['one']:\n",
    "        if i in list(alldata[(alldata['Method']=='DistilBERT_Partition') & (alldata['Freq']=='one')].Word):\n",
    "            d_1_wrong.append(i)\n",
    "    for i in former['DistilBERT_Partition']['wrong']['freq']:\n",
    "        if i in list(alldata[(alldata['Method']=='DistilBERT_Partition') & (alldata['Freq']=='multi')].Word):\n",
    "            d_m_wrong.append(i)\n",
    "    \n",
    "    for word in desB:\n",
    "        sub = alldata[(alldata[\"Word\"] == word) & (alldata['Method'] == 'Roberta_Partition')]\n",
    "        for ind, freq in enumerate(sub['Freq']):\n",
    "            if freq == 'one':\n",
    "                r_1_true.append(word)\n",
    "            else:\n",
    "                r_m_true.append(word)\n",
    "        \n",
    "        sub = alldata[(alldata[\"Word\"] == word) & (alldata['Method'] == 'DistilBERT_Partition')]\n",
    "        for ind, freq in enumerate(sub['Freq']):\n",
    "            if freq == 'one':\n",
    "                d_1_true.append(word)\n",
    "            else:\n",
    "                d_m_true.append(word)\n",
    "        \n",
    "\n",
    "    for word in no:\n",
    "        sub = alldata[(alldata[\"Word\"] == word) & (alldata['Method'] == 'Roberta_Partition')]\n",
    "        for ind, freq in enumerate(sub['Freq']):\n",
    "            if freq == 'one':\n",
    "                r_1_wrong.append(word)\n",
    "            else:\n",
    "                r_m_wrong.append(word)\n",
    "        \n",
    "        sub = alldata[(alldata[\"Word\"] == word) & (alldata['Method'] == 'DistilBERT_Partition')]\n",
    "        for ind, freq in enumerate(sub['Freq']):\n",
    "            if freq == 'one':\n",
    "                d_1_wrong.append(word)\n",
    "            else:\n",
    "                d_m_wrong.append(word)\n",
    "\n",
    "    former['Roberta_Partition']['right']['one'] = r_1_true\n",
    "    former['Roberta_Partition']['right']['freq'] = r_m_true\n",
    "    former['Roberta_Partition']['wrong']['one'] = r_1_wrong\n",
    "    former['Roberta_Partition']['wrong']['freq'] = r_m_wrong\n",
    "    former['DistilBERT_Partition']['right']['one'] = d_1_true\n",
    "    former['DistilBERT_Partition']['right']['freq'] = d_m_true\n",
    "    former['DistilBERT_Partition']['wrong']['one'] = d_1_wrong\n",
    "    former['DistilBERT_Partition']['wrong']['freq'] = d_m_wrong\n",
    "    \n",
    "    for method in ['Roberta_Partition', 'DistilBERT_Partition']:\n",
    "        print(method, len(former[method]['right']['one'])/total, len(former[method]['right']['freq'])/total)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(former, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "electoral-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv(\"topwords/all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-mattress",
   "metadata": {},
   "source": [
    "#### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indie-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivedata = alldata[alldata[\"Label\"] == \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sapphire-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('positive.csv')\n",
    "df2 = pd.read_csv('positive_2.csv')\n",
    "\n",
    "df_pos = pd.concat([df,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abstract-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "attention_checker_pos = {\"great\": 1, \"skiing\": 2, \"deadline\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "\n",
    "no_use_pos, df_valid_pos = valid_filter(df_pos, attention_checker_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noticed-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.446\n"
     ]
    }
   ],
   "source": [
    "worker_dataframe = worker_stat(df_pos, df_valid_pos, no_use_pos, 'Pos')\n",
    "calculate_mean_ck(df_valid_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frank-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 8.0 21.666666666666668\n",
      "Roberta_Partition 6.666666666666667 28.0\n",
      "DistilBERT_Mask 10.666666666666666 50.0\n",
      "DistilBERT_Partition 9.333333333333334 30.0\n",
      "FFN 28.333333333333332 63.666666666666664\n",
      "SVM 19.666666666666668 57.0\n",
      "LSTM 8.333333333333334 60.333333333333336\n",
      "Uni 5.666666666666667 46.0\n"
     ]
    }
   ],
   "source": [
    "lexica_pos, non_lexica_pos = lexicon_classification(df_valid_pos)\n",
    "generate_result(lexica_pos, non_lexica_pos, positivedata, \"../empathy_dictionary/lexica/AMT/results/positive.json\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f45bb264-b3f1-4dc7-bf86-00576673a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivedata = alldata[alldata[\"Label\"] == \"pos\"]\n",
    "df_pos = pd.read_csv('../../../amt_results/positive_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "930c2772-9733-405d-b715-7f593b3e43bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "attention_checker_pos = {\"great\": 1, \"skiing\": 2, \"deadline\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_pos, df_valid_pos = valid_filter(df_pos, attention_checker_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bf432de3-7990-4ab6-871a-c534a4bb7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.411\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f630a72f-ef8e-4bab-9361-bfd176dadf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 9.666666666666666 29.333333333333332\n",
      "DistilBERT_Partition 11.666666666666666 30.0\n"
     ]
    }
   ],
   "source": [
    "lexica_pos, non_lexica_pos = lexicon_classification(df_valid_pos)\n",
    "generate_result_new(lexica_pos, non_lexica_pos, positivedata, \"results/positive.json\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-partnership",
   "metadata": {},
   "source": [
    "#### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alone-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativedata = alldata[alldata[\"Label\"] == \"neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "imported-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv('negative.csv')\n",
    "df_neg = pd.concat([df_neg, pd.read_csv('negative_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hybrid-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "attention_checker_neg = {\"terrible\": 1, \"great\": 3, \"exam\": 2, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "\n",
    "no_use_neg, df_valid_neg = valid_filter(df_neg, attention_checker_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loved-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.431\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_neg, df_valid_neg, no_use_neg, 'Neg')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 16.333333333333332 48.666666666666664\n",
      "Roberta_Partition 11.0 41.333333333333336\n",
      "DistilBERT_Mask 24.333333333333332 62.0\n",
      "DistilBERT_Partition 10.666666666666666 40.666666666666664\n",
      "FFN 46.0 63.666666666666664\n",
      "SVM 42.333333333333336 61.666666666666664\n",
      "LSTM 14.666666666666666 58.666666666666664\n",
      "Uni 8.333333333333334 19.666666666666668\n"
     ]
    }
   ],
   "source": [
    "lexica_neg, non_lexica_neg = lexicon_classification(df_valid_neg)\n",
    "generate_result(lexica_neg, non_lexica_neg, negativedata, \"../empathy_dictionary/lexica/AMT/results/negative.json\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e1b0ff9-3909-416f-89a8-54368c984e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativedata = alldata[alldata[\"Label\"] == \"neg\"]\n",
    "df_neg = pd.read_csv('../../../amt_results/negative_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "557502c3-8180-4616-b6d1-61b9f439f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "attention_checker_neg = {\"terrible\": 1, \"great\": 3, \"exam\": 2, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "\n",
    "no_use_neg, df_valid_neg = valid_filter(df_neg, attention_checker_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20103861-e5f2-471f-a4b2-74a8a53c820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.494\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c3343458-fe9a-4e5c-a897-17affa486d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 20.666666666666668 42.333333333333336\n",
      "DistilBERT_Partition 12.666666666666666 42.0\n"
     ]
    }
   ],
   "source": [
    "lexica_neg, non_lexica_neg = lexicon_classification(df_valid_neg)\n",
    "generate_result_new(lexica_neg, non_lexica_neg, negativedata, \"results/negative.json\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-mortality",
   "metadata": {},
   "source": [
    "#### Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "genetic-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "joydata = alldata[alldata[\"Label\"] == \"joy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "criminal-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy = pd.read_csv('joy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "third-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "attention_checker_joy = {'happy':1, 'party':2, 'jail':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_joy, df_valid_joy = valid_filter(df_joy, attention_checker_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stone-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.511\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_joy, df_valid_joy, no_use_joy, 'Joy')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "moral-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 18.0 25.0\n",
      "Roberta_Partition 24.0 21.0\n",
      "DistilBERT_Mask 16.0 31.0\n",
      "DistilBERT_Partition 12.0 15.0\n",
      "FFN 21.0 39.0\n",
      "SVM 16.0 38.0\n",
      "LSTM 11.0 25.0\n",
      "Uni 6.0 19.0\n"
     ]
    }
   ],
   "source": [
    "lexica_joy, non_lexica_joy = lexicon_classification(df_valid_joy)\n",
    "generate_result(lexica_joy, non_lexica_joy, joydata, \"../empathy_dictionary/lexica/AMT/results/joy.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "574bceb8-d026-4c1c-9869-edf9db3f6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "joydata = alldata[alldata[\"Label\"] == \"joy\"]\n",
    "df_joy = pd.read_csv('../../../amt_results/joy_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "687f62f4-4ac1-4aae-8aa2-dd31b168eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "attention_checker_joy = {'happy':1, 'party':2, 'jail':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_joy, df_valid_joy = valid_filter(df_joy, attention_checker_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49006c78-1910-4083-a3cc-09f0d90f5673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.537\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dee2fbb1-9395-444f-99d2-75d3950b8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 29.0 29.0\n",
      "DistilBERT_Partition 18.0 22.0\n"
     ]
    }
   ],
   "source": [
    "lexica_joy, non_lexica_joy = lexicon_classification(df_valid_joy)\n",
    "generate_result_new(lexica_joy, non_lexica_joy, joydata, \"results/joy.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-organic",
   "metadata": {},
   "source": [
    "#### Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exceptional-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "angerdata = alldata[alldata[\"Label\"] == \"anger\"]\n",
    "df_anger = pd.read_csv('anger.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "creative-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "attention_checker_anger = {'angry':1, 'argument':2, 'animal':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_anger, df_valid_anger = valid_filter(df_anger, attention_checker_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "graphic-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.551\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_anger, df_valid_anger, no_use_anger, 'Anger')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alive-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 3.0 14.0\n",
      "Roberta_Partition 9.0 18.0\n",
      "DistilBERT_Mask 19.0 19.0\n",
      "DistilBERT_Partition 6.0 20.0\n",
      "FFN 19.0 15.0\n",
      "SVM 15.0 16.0\n",
      "LSTM 12.0 18.0\n",
      "Uni 0.0 13.0\n"
     ]
    }
   ],
   "source": [
    "lexica_anger, non_lexica_anger = lexicon_classification(df_valid_anger)\n",
    "generate_result(lexica_anger, non_lexica_anger, angerdata, \"../empathy_dictionary/lexica/AMT/results/anger.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10a2162f-c0c7-40e7-bf8f-1d665e9070f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "angerdata = alldata[alldata[\"Label\"] == \"anger\"]\n",
    "df_anger = pd.read_csv('../../../amt_results/anger_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0a46b4a-9d0f-49ff-a646-004cfde6d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "attention_checker_anger = {'angry':1, 'argument':2, 'animal':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_anger, df_valid_anger = valid_filter(df_anger, attention_checker_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bfd76c16-6238-411f-9f3e-790bd18a741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.655\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d062e36-ab66-46bb-a1bf-886eb3cbb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 11.0 17.0\n",
      "DistilBERT_Partition 10.0 21.0\n"
     ]
    }
   ],
   "source": [
    "lexica_anger, non_lexica_anger = lexicon_classification(df_valid_anger)\n",
    "generate_result_new(lexica_anger, non_lexica_anger, angerdata, \"results/anger.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-being",
   "metadata": {},
   "source": [
    "#### Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excited-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "feardata = alldata[alldata[\"Label\"] == \"fear\"]\n",
    "df_fear = pd.read_csv('fear.csv')\n",
    "df_fear = pd.concat([df_fear, pd.read_csv('fear_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "banner-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "attention_checker_fear = {\"afraid\": 1, \"jail\": 2, \"table\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_fear, df_valid_fear = valid_filter(df_fear, attention_checker_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "olympic-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.557\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_fear, df_valid_fear, no_use_fear, 'Fear')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "protected-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 14.0 28.0\n",
      "Roberta_Partition 14.0 29.0\n",
      "DistilBERT_Mask 25.0 33.0\n",
      "DistilBERT_Partition 2.0 18.0\n",
      "FFN 28.0 28.0\n",
      "SVM 35.0 31.0\n",
      "LSTM 18.0 30.0\n",
      "Uni 3.0 14.0\n"
     ]
    }
   ],
   "source": [
    "lexica_fear, non_lexica_fear = lexicon_classification(df_valid_fear)\n",
    "generate_result(lexica_fear, non_lexica_fear, feardata, \"../empathy_dictionary/lexica/AMT/results/fear.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "509ca0a0-e416-4f11-9e1c-33d328f9deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feardata = alldata[alldata[\"Label\"] == \"fear\"]\n",
    "df_fear = pd.read_csv('../../../amt_results/fear_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c7f9651-7863-494d-b72a-260ae9300b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "attention_checker_fear = {\"afraid\": 1, \"jail\": 2, \"table\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_fear, df_valid_fear = valid_filter(df_fear, attention_checker_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "237ac095-e9c0-44c8-92f2-a913524c5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.502\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e88739f2-0212-462d-8710-b278852c37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 14.0 32.0\n",
      "DistilBERT_Partition 6.0 20.0\n"
     ]
    }
   ],
   "source": [
    "lexica_fear, non_lexica_fear = lexicon_classification(df_valid_fear)\n",
    "generate_result_new(lexica_fear, non_lexica_fear, feardata, \"results/fear.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-strand",
   "metadata": {},
   "source": [
    "#### Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "complimentary-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "saddata = alldata[alldata[\"Label\"] == \"sadness\"]\n",
    "df_sad = pd.read_csv('sadness.csv')\n",
    "df_sad = pd.concat([df_sad, pd.read_csv('sadness_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "labeled-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "attention_checker_sad = {\"sad\": 1, \"tear\": 2, \"party\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_sad, df_valid_sad = valid_filter(df_sad, attention_checker_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "better-position",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.576\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_sad, df_valid_sad, no_use_sad, 'Sadness')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "balanced-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 8.0 22.0\n",
      "Roberta_Partition 3.0 18.0\n",
      "DistilBERT_Mask 8.0 18.0\n",
      "DistilBERT_Partition 2.0 14.0\n",
      "FFN 6.0 17.0\n",
      "SVM 8.0 17.0\n",
      "LSTM 7.0 17.0\n",
      "Uni 1.0 13.0\n"
     ]
    }
   ],
   "source": [
    "lexica_sad, non_lexica_sad = lexicon_classification(df_valid_sad)\n",
    "generate_result(lexica_sad, non_lexica_sad, saddata, \"../empathy_dictionary/lexica/AMT/results/sad.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad66a2a9-724f-4d0f-804e-f2d2252e14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "saddata = alldata[alldata[\"Label\"] == \"sadness\"]\n",
    "df_sad = pd.read_csv('../../../amt_results/sadness_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91a04f67-2d3e-4f2d-bf7c-056d93f14bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "attention_checker_sad = {\"sad\": 1, \"tear\": 2, \"party\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_sad, df_valid_sad = valid_filter(df_sad, attention_checker_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46791ccf-154f-4a84-8e52-ff715173f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.555\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8c15b6a7-9bcb-437f-958b-f26ac5fc96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 2.0 23.0\n",
      "DistilBERT_Partition 2.0 18.0\n"
     ]
    }
   ],
   "source": [
    "lexica_sad, non_lexica_sad = lexicon_classification(df_valid_sad)\n",
    "generate_result_new(lexica_sad, non_lexica_sad, saddata, \"results/sad.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-antenna",
   "metadata": {},
   "source": [
    "#### surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "parliamentary-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisedata = alldata[alldata[\"Label\"] == \"surprise\"]\n",
    "df_surprise = pd.read_csv('surprise.csv')\n",
    "df_surprise = pd.concat([df_surprise, pd.read_csv('surprise_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "experimental-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "attention_checker_surprise = {\"surprising\": 1, \"magician\": 2, \"book\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_surprise, df_valid_surprise = valid_filter(df_surprise, attention_checker_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "optimum-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.517\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_surprise, df_valid_surprise, no_use_surprise, 'Surprise')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fossil-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 4.0 9.0\n",
      "Roberta_Partition 5.0 13.0\n",
      "DistilBERT_Mask 3.0 11.0\n",
      "DistilBERT_Partition 2.0 9.0\n",
      "FFN 9.0 11.0\n",
      "SVM 8.0 11.0\n",
      "LSTM 9.0 15.0\n",
      "Uni 1.0 6.0\n"
     ]
    }
   ],
   "source": [
    "lexica_surprise, non_lexica_surprise = lexicon_classification(df_valid_surprise)\n",
    "generate_result(lexica_surprise, non_lexica_surprise, surprisedata, \"../empathy_dictionary/lexica/AMT/results/surprise.json\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07d26bd3-d609-4eca-9fc9-34a678e8b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisedata = alldata[alldata[\"Label\"] == \"surprise\"]\n",
    "df_surprise = pd.read_csv('../../../amt_results/surprise_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48829d3b-155c-4618-b20f-99cb55099b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "attention_checker_surprise = {\"surprising\": 1, \"magician\": 2, \"book\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_surprise, df_valid_surprise = valid_filter(df_surprise, attention_checker_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3155afcd-3d82-4e0a-870a-68acc9f6138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.532\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_ck(df_valid_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b48a18a-343f-410b-94e1-f2af0662923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Partition 4.0 15.0\n",
      "DistilBERT_Partition 3.0 11.0\n"
     ]
    }
   ],
   "source": [
    "lexica_surprise, non_lexica_surprise = lexicon_classification(df_valid_surprise)\n",
    "generate_result_new(lexica_surprise, non_lexica_surprise, surprisedata, \"results/surprise.json\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rating]",
   "language": "python",
   "name": "conda-env-rating-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
