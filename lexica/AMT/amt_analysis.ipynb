{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infectious-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "young-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_filter(df, attention_checker):\n",
    "    not_use = 0\n",
    "    index = []\n",
    "    for i in range(len(df)):\n",
    "        sub = df.iloc[i]\n",
    "        not_pass = 0\n",
    "        for word in range(1,61):\n",
    "            input_col = 'Input.word'+str(word)\n",
    "            word_col = 'Answer.word'+str(word)+'-radios'\n",
    "            if sub[input_col] in attention_checker:\n",
    "                if sub[word_col] != attention_checker[sub[input_col]]:\n",
    "                    not_pass += 1\n",
    "        if not_pass > 2:\n",
    "            index.append(i)\n",
    "            not_use += 1\n",
    "\n",
    "    print(df.shape[0])\n",
    "    print(not_use)\n",
    "    #notuse.append(not_use)\n",
    "\n",
    "    df_valid = []\n",
    "    for i in range(len(index)):\n",
    "        if i != len(index)-1:\n",
    "            df_valid.append(df.iloc[index[i]+1:index[i+1]])\n",
    "        else:\n",
    "            if index[i] != len(df) - 1:\n",
    "                df_valid.append(df.iloc[index[i]+1:])\n",
    "\n",
    "    df_valid = pd.concat(df_valid)\n",
    "    \n",
    "    return index, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regulation-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_stat(df, df_valid, index, task):\n",
    "    worker_mean_kappa = {}\n",
    "    for worker in df[\"WorkerId\"].unique():\n",
    "        workersub = df[df[\"WorkerId\"] == worker]\n",
    "        if worker not in worker_mean_kappa:\n",
    "            worker_mean_kappa[worker] = [len(workersub)]\n",
    "        hits = workersub.HITId.unique()\n",
    "        for hit in hits:\n",
    "            workerhit = workersub[workersub[\"HITId\"] == hit]\n",
    "            hitsub = df_valid[df_valid['HITId'] == hit]\n",
    "            if len(hitsub) <= 1:\n",
    "                continue\n",
    "            word_col = []\n",
    "            for word in range(1,61):\n",
    "                word_col.append('Answer.word'+str(word)+'-radios')\n",
    "            for i in range(len(hitsub)):\n",
    "                if hitsub.iloc[i][\"WorkerId\"] != worker:\n",
    "                    score = cohen_kappa_score(hitsub.iloc[i][word_col].astype(int), workerhit.iloc[0][word_col].astype(int), labels=[1,2,3])\n",
    "                    if np.isnan(score):\n",
    "                        score = 1\n",
    "                    worker_mean_kappa[worker].append(score)\n",
    "\n",
    "    worker_list = {}\n",
    "    for worker in worker_mean_kappa:\n",
    "        if worker not in worker_list:\n",
    "            if len(worker_mean_kappa[worker]) == 1:\n",
    "                worker_list[worker] = [worker_mean_kappa[worker][0], 0, np.nan, 0, 0]\n",
    "            else:\n",
    "                mean = round(np.mean(worker_mean_kappa[worker][1:]), 3)\n",
    "                bad_times = sum([i<0.3 for i in worker_mean_kappa[worker][1:]])\n",
    "                good_times = sum([i>0.55 for i in worker_mean_kappa[worker][1:]])\n",
    "#                 print(worker, worker_mean_kappa[worker][1:], mean, bad_times, good_times)\n",
    "                worker_list[worker] = [worker_mean_kappa[worker][0], 0, mean, bad_times, good_times]\n",
    "\n",
    "    temp = {}\n",
    "    for worker in df.iloc[index].WorkerId:\n",
    "        if worker in temp:\n",
    "            temp[worker] += 1\n",
    "        else:\n",
    "            temp[worker] = 1\n",
    "\n",
    "    for worker in temp:\n",
    "        if temp[worker]/worker_list[worker][0]>0.3:\n",
    "            worker_list[worker][1] = 1\n",
    "\n",
    "    worker_data = pd.DataFrame(worker_list).T\n",
    "    worker_data.columns = [task+i for i in [\" HITs Count\", \" No Pass\", \" Mean Kappa\", \" Bad Times\", \" Good Times\"]]\n",
    "    \n",
    "    return worker_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indoor-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_ck(df_valid):\n",
    "    ck = []\n",
    "    for hit in df_valid['HITId'].unique():\n",
    "        sub = df_valid[df_valid['HITId'] == hit]\n",
    "        if len(sub) <= 1:\n",
    "            continue\n",
    "        word_col = []\n",
    "        for word in range(1,61):\n",
    "            word_col.append('Answer.word'+str(word)+'-radios')\n",
    "        for i in range(len(sub)-1):\n",
    "            for j in range(i+1, len(sub)):\n",
    "                score = cohen_kappa_score(sub.iloc[i][word_col].astype(int), sub.iloc[j][word_col].astype(int), labels=[1,2,3])\n",
    "                if np.isnan(score):\n",
    "                    score = 1\n",
    "                ck.append(score)\n",
    "\n",
    "    print('Average CK:{:.3f}'.format(np.mean(ck)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "european-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_classification(df_valid, threshold=0.8):\n",
    "    word2score = {}\n",
    "    for answer_ind in range(len(df_valid)):\n",
    "        answer = df_valid.iloc[answer_ind]\n",
    "        for word in range(1,61):\n",
    "            input_col = 'Input.word'+str(word)\n",
    "            word_col = 'Answer.word'+str(word)+'-radios'\n",
    "            #score = max(3 - answer[word_col].astype(int), 0)\n",
    "            score = 1 if answer[word_col].astype(int) < 3 else 0\n",
    "            if answer[input_col] not in word2score:\n",
    "                word2score[answer[input_col]] = [score, 1]\n",
    "            else:\n",
    "                word2score[answer[input_col]][0] = word2score[answer[input_col]][0]+score\n",
    "                word2score[answer[input_col]][1] = word2score[answer[input_col]][1]+1\n",
    "                \n",
    "    desB = []\n",
    "    # relB = []\n",
    "    no = []\n",
    "\n",
    "    for word in word2score:\n",
    "        average = word2score[word][0]/word2score[word][1]\n",
    "        #if average > 1.6:\n",
    "        if average >= threshold:\n",
    "            desB.append(word)\n",
    "        #elif average < 0.6:\n",
    "        else:\n",
    "            no.append(word)\n",
    "        #else:\n",
    "        #    relB.append(word)\n",
    "                \n",
    "    return desB, no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "synthetic-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(desB, no, positivedata, filename, total):\n",
    "    stat = {}\n",
    "    for method in alldata.Method.unique():\n",
    "        stat[method] = {\"right\": {\"one\":[], \"freq\":[]}, \"wrong\":{\"one\":[], \"freq\":[]}}\n",
    "\n",
    "    for word in desB:\n",
    "        sub = positivedata[positivedata[\"Word\"] == word]\n",
    "        for ind, method in enumerate(sub[\"Method\"]):\n",
    "            if sub.iloc[ind][\"Freq\"] == \"one\":\n",
    "                stat[method][\"right\"][\"one\"].append(word)\n",
    "            else:\n",
    "                stat[method][\"right\"][\"freq\"].append(word)\n",
    "\n",
    "    for word in no:\n",
    "        sub = positivedata[positivedata[\"Word\"] == word]\n",
    "        for ind, method in enumerate(sub[\"Method\"]):\n",
    "            if sub.iloc[ind][\"Freq\"] == \"one\":\n",
    "                stat[method][\"wrong\"][\"one\"].append(word)\n",
    "            else:\n",
    "                stat[method][\"wrong\"][\"freq\"].append(word)\n",
    "\n",
    "    for method in stat:\n",
    "        print(method, len(stat[method]['right']['one'])/total, len(stat[method]['right']['freq'])/total)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(stat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electoral-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv(\"../empathy_dictionary/lexica/AMT/topwords/all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-mattress",
   "metadata": {},
   "source": [
    "#### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indie-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivedata = alldata[alldata[\"Label\"] == \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sapphire-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('positive.csv')\n",
    "df2 = pd.read_csv('positive_2.csv')\n",
    "\n",
    "df_pos = pd.concat([df,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abstract-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "attention_checker_pos = {\"great\": 1, \"skiing\": 2, \"deadline\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "\n",
    "no_use_pos, df_valid_pos = valid_filter(df_pos, attention_checker_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noticed-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.446\n"
     ]
    }
   ],
   "source": [
    "worker_dataframe = worker_stat(df_pos, df_valid_pos, no_use_pos, 'Pos')\n",
    "calculate_mean_ck(df_valid_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frank-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 8.0 21.666666666666668\n",
      "Roberta_Partition 6.666666666666667 28.0\n",
      "DistilBERT_Mask 10.666666666666666 50.0\n",
      "DistilBERT_Partition 9.333333333333334 30.0\n",
      "FFN 28.333333333333332 63.666666666666664\n",
      "SVM 19.666666666666668 57.0\n",
      "LSTM 8.333333333333334 60.333333333333336\n",
      "Uni 5.666666666666667 46.0\n"
     ]
    }
   ],
   "source": [
    "lexica_pos, non_lexica_pos = lexicon_classification(df_valid_pos)\n",
    "generate_result(lexica_pos, non_lexica_pos, positivedata, \"../empathy_dictionary/lexica/AMT/results/positive.json\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-partnership",
   "metadata": {},
   "source": [
    "#### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alone-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativedata = alldata[alldata[\"Label\"] == \"neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "imported-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv('negative.csv')\n",
    "df_neg = pd.concat([df_neg, pd.read_csv('negative_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hybrid-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "attention_checker_neg = {\"terrible\": 1, \"great\": 3, \"exam\": 2, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "\n",
    "no_use_neg, df_valid_neg = valid_filter(df_neg, attention_checker_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loved-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.431\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_neg, df_valid_neg, no_use_neg, 'Neg')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 16.333333333333332 48.666666666666664\n",
      "Roberta_Partition 11.0 41.333333333333336\n",
      "DistilBERT_Mask 24.333333333333332 62.0\n",
      "DistilBERT_Partition 10.666666666666666 40.666666666666664\n",
      "FFN 46.0 63.666666666666664\n",
      "SVM 42.333333333333336 61.666666666666664\n",
      "LSTM 14.666666666666666 58.666666666666664\n",
      "Uni 8.333333333333334 19.666666666666668\n"
     ]
    }
   ],
   "source": [
    "lexica_neg, non_lexica_neg = lexicon_classification(df_valid_neg)\n",
    "generate_result(lexica_neg, non_lexica_neg, negativedata, \"../empathy_dictionary/lexica/AMT/results/negative.json\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-mortality",
   "metadata": {},
   "source": [
    "#### Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "genetic-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "joydata = alldata[alldata[\"Label\"] == \"joy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "criminal-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joy = pd.read_csv('joy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "third-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "attention_checker_joy = {'happy':1, 'party':2, 'jail':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_joy, df_valid_joy = valid_filter(df_joy, attention_checker_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stone-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.511\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_joy, df_valid_joy, no_use_joy, 'Joy')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_joy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "moral-comment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 18.0 25.0\n",
      "Roberta_Partition 24.0 21.0\n",
      "DistilBERT_Mask 16.0 31.0\n",
      "DistilBERT_Partition 12.0 15.0\n",
      "FFN 21.0 39.0\n",
      "SVM 16.0 38.0\n",
      "LSTM 11.0 25.0\n",
      "Uni 6.0 19.0\n"
     ]
    }
   ],
   "source": [
    "lexica_joy, non_lexica_joy = lexicon_classification(df_valid_joy)\n",
    "generate_result(lexica_joy, non_lexica_joy, joydata, \"../empathy_dictionary/lexica/AMT/results/joy.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-organic",
   "metadata": {},
   "source": [
    "#### Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exceptional-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "angerdata = alldata[alldata[\"Label\"] == \"anger\"]\n",
    "df_anger = pd.read_csv('anger.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "creative-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "attention_checker_anger = {'angry':1, 'argument':2, 'animal':3, 'further':3, 'the':3, 'alsike':4, 'Q<--->':4}\n",
    "no_use_anger, df_valid_anger = valid_filter(df_anger, attention_checker_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "graphic-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.551\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_anger, df_valid_anger, no_use_anger, 'Anger')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alive-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 3.0 14.0\n",
      "Roberta_Partition 9.0 18.0\n",
      "DistilBERT_Mask 19.0 19.0\n",
      "DistilBERT_Partition 6.0 20.0\n",
      "FFN 19.0 15.0\n",
      "SVM 15.0 16.0\n",
      "LSTM 12.0 18.0\n",
      "Uni 0.0 13.0\n"
     ]
    }
   ],
   "source": [
    "lexica_anger, non_lexica_anger = lexicon_classification(df_valid_anger)\n",
    "generate_result(lexica_anger, non_lexica_anger, angerdata, \"../empathy_dictionary/lexica/AMT/results/anger.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-being",
   "metadata": {},
   "source": [
    "#### Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excited-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "feardata = alldata[alldata[\"Label\"] == \"fear\"]\n",
    "df_fear = pd.read_csv('fear.csv')\n",
    "df_fear = pd.concat([df_fear, pd.read_csv('fear_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "banner-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "attention_checker_fear = {\"afraid\": 1, \"jail\": 2, \"table\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_fear, df_valid_fear = valid_filter(df_fear, attention_checker_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "olympic-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.557\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_fear, df_valid_fear, no_use_fear, 'Fear')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "protected-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 14.0 28.0\n",
      "Roberta_Partition 14.0 29.0\n",
      "DistilBERT_Mask 25.0 33.0\n",
      "DistilBERT_Partition 2.0 18.0\n",
      "FFN 28.0 28.0\n",
      "SVM 35.0 31.0\n",
      "LSTM 18.0 30.0\n",
      "Uni 3.0 14.0\n"
     ]
    }
   ],
   "source": [
    "lexica_fear, non_lexica_fear = lexicon_classification(df_valid_fear)\n",
    "generate_result(lexica_fear, non_lexica_fear, feardata, \"../empathy_dictionary/lexica/AMT/results/fear.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-strand",
   "metadata": {},
   "source": [
    "#### Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "complimentary-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "saddata = alldata[alldata[\"Label\"] == \"sadness\"]\n",
    "df_sad = pd.read_csv('sadness.csv')\n",
    "df_sad = pd.concat([df_sad, pd.read_csv('sadness_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "labeled-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "attention_checker_sad = {\"sad\": 1, \"tear\": 2, \"party\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_sad, df_valid_sad = valid_filter(df_sad, attention_checker_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "better-position",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.576\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_sad, df_valid_sad, no_use_sad, 'Sadness')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "balanced-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 8.0 22.0\n",
      "Roberta_Partition 3.0 18.0\n",
      "DistilBERT_Mask 8.0 18.0\n",
      "DistilBERT_Partition 2.0 14.0\n",
      "FFN 6.0 17.0\n",
      "SVM 8.0 17.0\n",
      "LSTM 7.0 17.0\n",
      "Uni 1.0 13.0\n"
     ]
    }
   ],
   "source": [
    "lexica_sad, non_lexica_sad = lexicon_classification(df_valid_sad)\n",
    "generate_result(lexica_sad, non_lexica_sad, saddata, \"../empathy_dictionary/lexica/AMT/results/sad.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-antenna",
   "metadata": {},
   "source": [
    "#### surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "parliamentary-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisedata = alldata[alldata[\"Label\"] == \"surprise\"]\n",
    "df_surprise = pd.read_csv('surprise.csv')\n",
    "df_surprise = pd.concat([df_surprise, pd.read_csv('surprise_2.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "experimental-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "attention_checker_surprise = {\"surprising\": 1, \"magician\": 2, \"book\": 3, \"further\": 3, \"the\": 3, \"alsike\": 4, \"Q<--->\": 4}\n",
    "no_use_surprise, df_valid_surprise = valid_filter(df_surprise, attention_checker_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "optimum-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CK:0.517\n"
     ]
    }
   ],
   "source": [
    "worker_data = worker_stat(df_surprise, df_valid_surprise, no_use_surprise, 'Surprise')\n",
    "worker_dataframe = worker_dataframe.merge(worker_data, left_index=True, right_index=True, how=\"outer\")\n",
    "calculate_mean_ck(df_valid_surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fossil-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta_Mask 4.0 9.0\n",
      "Roberta_Partition 5.0 13.0\n",
      "DistilBERT_Mask 3.0 11.0\n",
      "DistilBERT_Partition 2.0 9.0\n",
      "FFN 9.0 11.0\n",
      "SVM 8.0 11.0\n",
      "LSTM 9.0 15.0\n",
      "Uni 1.0 6.0\n"
     ]
    }
   ],
   "source": [
    "lexica_surprise, non_lexica_surprise = lexicon_classification(df_valid_surprise)\n",
    "generate_result(lexica_surprise, non_lexica_surprise, surprisedata, \"../empathy_dictionary/lexica/AMT/results/surprise.json\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rating]",
   "language": "python",
   "name": "conda-env-rating-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
