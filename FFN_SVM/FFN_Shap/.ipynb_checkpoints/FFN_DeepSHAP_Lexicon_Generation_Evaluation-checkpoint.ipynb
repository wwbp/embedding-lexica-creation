{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wr6fHCNvf3ol"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/data1/YelpAnalysis/')\n",
    "from utils import *\n",
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')\n",
    "\n",
    "sys.path.insert(0,'/data2/Datasets/')\n",
    "from preprocess import *\n",
    "\n",
    "dataFolder = '/data2/Datasets/Raw'\n",
    "device = 'cuda:2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "IRI_-_R4gC0J",
    "outputId": "2198398f-8497-4fbd-e9ac-bf484bc6ed78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 126.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.5 F1 : 0.6666666666666666\n",
      "*************************\n",
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 144.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.6210526315789474 F1 : 0.6538461538461539\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 170.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.5736842105263158 F1 : 0.4335664335664336\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE TRAINING DATASET FOR LEXICON CREATION HERE\n",
    "# nrc_joy, nrc_sadness, nec_surprise, nrc_fear, nrc_anger,\n",
    "# empathy, yelp_subset, amazon_finefood_subset, amazon_toys_subset\n",
    "\n",
    "lexiconDataset = \"empathy\"\n",
    "trainDf, devDf, testDf = splitData(getData(dataFolder,lexiconDataset))\n",
    "\n",
    "# Preparing Data\n",
    "trainData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "\n",
    "testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "\n",
    "trainDataset = Dataset(trainDf, trainData)\n",
    "testDataset = Dataset(testDf, testData)\n",
    "\n",
    "# Training NN\n",
    "NNnet = trainFFN(trainDataset, testDataset, num_epochs = 3, batchSize = 5, device =\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 300)\n"
     ]
    }
   ],
   "source": [
    "print((trainData).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count number of unique words in a dataset\n",
    "def getWordCount(data, textVariable = 'text'):\n",
    "    vectorizer = CountVectorizer(stop_words=[], tokenizer= nltk.word_tokenize)\n",
    "    vectorizer.fit(data[textVariable])\n",
    "    cvFit = vectorizer.transform(data[textVariable])\n",
    "    wordList = list(vectorizer.vocabulary_.keys())\n",
    "    counts = np.asarray(cvFit.sum(axis=0))[0]\n",
    "\n",
    "    wordCount = []\n",
    "    for i in range(len(wordList)):\n",
    "        wordCount.append(counts[vectorizer.vocabulary_[wordList[i]]])\n",
    "    df = pd.DataFrame({'word':wordList,'wordCount':wordCount})    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_df = getWordCount(trainDf)\n",
    "count_test_df = getWordCount(testDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081\n",
      "2890\n"
     ]
    }
   ],
   "source": [
    "print(len(count_train_df))\n",
    "print(len(count_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"/data2/link10/models/fasttext/en_fasttext_crawl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a count of words in the dataset, tensor representation for dataset, \n",
    "# word frequency for the words, and a mapping from a word to the the review IDs in which the \n",
    "# word is present\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer1 = Tokenizer(nlp1.vocab)\n",
    "\n",
    "def getWordVectors(dataframe, data_name = \"text\"):\n",
    "    init_cnt=0\n",
    "    freq_dict = {}\n",
    "    occur_dict ={}\n",
    "    review_idx=-1\n",
    "    with nlp1.disable_pipes():\n",
    "        for msg in tqdm(dataframe[data_name]): \n",
    "            review_idx+=1\n",
    "            for word in tokenizer1(msg.lower()):\n",
    "        \n",
    "                if(str(word)) not in occur_dict:\n",
    "                    occur_dict[str(word)]=list()\n",
    "                occur_dict[str(word)].append(review_idx)\n",
    "                \n",
    "                \n",
    "                if str(word) in freq_dict:\n",
    "                    freq_dict[str(word)] += 1\n",
    "                else:\n",
    "                    freq_dict.update({str(word): 1}) \n",
    "                    \n",
    "                init_cnt+=1\n",
    "                \n",
    "        assert(review_idx==len(dataframe)-1)\n",
    "            \n",
    "        print(init_cnt);\n",
    "        bg = torch.empty((init_cnt,300))\n",
    "        curr_cnt=0\n",
    "        for msg in tqdm(dataframe[data_name]):\n",
    "            for word in tokenizer1(msg.lower()):\n",
    "                word_vec = nlp1(str(word)).vector\n",
    "                curr_embed = torch.from_numpy(word_vec)\n",
    "                curr_embed = curr_embed.reshape((1, len(curr_embed)))\n",
    "                bg[curr_cnt] = curr_embed\n",
    "                curr_cnt+=1\n",
    "                \n",
    "        return init_cnt, bg, freq_dict, occur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNNet(\n",
      "  (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "(1536, 300)\n"
     ]
    }
   ],
   "source": [
    "print(NNnet)\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData = trainData[:10]\n",
    "train_bg = torch.FloatTensor(trainData)\n",
    "train_bg = train_bg.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = NNnet(train_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = op.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 300])\n"
     ]
    }
   ],
   "source": [
    "print(train_bg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bg =  train_bg[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Explainer object with the training data as background\n",
    "e = shap.DeepExplainer(NNnet, train_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating SHAP values\n",
    "st = time.time()\n",
    "shap_vals = e.shap_values(train_bg)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_vals[0])\n",
    "print(shap_vals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "7O1khsu9b4Nd",
    "outputId": "d0119827-64de-481e-e5a7-6ad86ab09f66"
   },
   "outputs": [],
   "source": [
    "print((shap_vals[1].shape))\n",
    "shap_scores = shap_vals[1].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "tE0UZdh5__8l",
    "outputId": "5c768f7c-a149-4d59-f4d1-cd7df79ce7ba"
   },
   "outputs": [],
   "source": [
    "explain_cnt,explain_examples, explain_dict, explain_occur = getWordVectors(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explain_examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(explain_occur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuLbh7GHJ4f9"
   },
   "outputs": [],
   "source": [
    "# Function which finds the shap score for each word/token\n",
    "# by averaging the shap scores of all the reviews in which the word occurs\n",
    "\n",
    "def findShapForToken(shap_scores, explain_occur, explain_dict):\n",
    "    explain_scores={}\n",
    "    for word in explain_occur:\n",
    "        indices = explain_occur[word]\n",
    "        word_score = 0\n",
    "        assert(explain_dict[word]==len(indices))\n",
    "        for index in indices:\n",
    "            if(index>=len(explain_occur)):\n",
    "                print(word)\n",
    "            word_score += shap_scores[index]\n",
    "        explain_scores[word] = word_score/len(indices)\n",
    "    return explain_scores       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X38r6yCXc3WW"
   },
   "outputs": [],
   "source": [
    " word_scores =findShapForToken(shap_scores, explain_occur, explain_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRYOx4w7l5Un"
   },
   "outputs": [],
   "source": [
    "# Converting into final lexicon format\n",
    "final_data = []\n",
    "for token in word_scores:\n",
    "    curr_token_data=[]\n",
    "    curr_token_data.append(token)\n",
    "    curr_token_data.append(word_scores[token])\n",
    "    curr_token_data.append(explain_dict[token])\n",
    "    final_data.append(curr_token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =  pd.DataFrame(final_data, columns =['word', 'score', 'word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE LEXICON SAVING DESTINATION HERE\n",
    "\n",
    "final_df.to_csv(path_or_buf=\"./FINAL_LEX/ffn_deepshap_nrc_anger_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/home/tjss/Final/embedding-lexica-creation/lexica/FFN_DeepShap/nrc_anger_ffn_deepshap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to be evaluated against\n",
    "\n",
    "dataList = ['nrc_anger', 'song_anger', 'dialog_anger', 'friends_anger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconWords, lexiconMap = getLexicon(df = final_df)\n",
    "\n",
    "results = []\n",
    "\n",
    "for data in dataList:\n",
    "    results.append(testFFN(NNnet,data,lexiconWords, lexiconMap, nlp, dataFolder))\n",
    "    \n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['TestData','modelAcc', 'modelF1', 'lexiconAcc', 'lexiconF1']\n",
    "results.to_csv(\"Results.csv\",index = False, index_label = False)\n",
    "print(\"--------------------\"+lexiconDataset+\"-----------------------------\")\n",
    "print(str(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "MeanEmbeddingsFasttext_NRCJoyVsNotJoy_wordLevelBackground(All Reviews)Shap.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
