{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wr6fHCNvf3ol"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/data1/YelpAnalysis/')\n",
    "from utils import *\n",
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')\n",
    "\n",
    "sys.path.insert(0,'/data2/Datasets/')\n",
    "from preprocess import *\n",
    "\n",
    "dataFolder = '/data2/Datasets/Raw'\n",
    "device = 'cuda:2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "IRI_-_R4gC0J",
    "outputId": "2198398f-8497-4fbd-e9ac-bf484bc6ed78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 126.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.5 F1 : 0.6666666666666666\n",
      "*************************\n",
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 144.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.6210526315789474 F1 : 0.6538461538461539\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 170.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.5736842105263158 F1 : 0.4335664335664336\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE TRAINING DATASET FOR LEXICON CREATION HERE\n",
    "# nrc_joy, nrc_sadness, nec_surprise, nrc_fear, nrc_anger,\n",
    "# empathy, yelp_subset, amazon_finefood_subset, amazon_toys_subset\n",
    "\n",
    "lexiconDataset = \"empathy\"\n",
    "trainDf, devDf, testDf = splitData(getData(dataFolder,lexiconDataset))\n",
    "\n",
    "# Preparing Data\n",
    "trainData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "\n",
    "testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "\n",
    "trainDataset = Dataset(trainDf, trainData)\n",
    "testDataset = Dataset(testDf, testData)\n",
    "\n",
    "# Training NN\n",
    "NNnet = trainFFN(trainDataset, testDataset, num_epochs = 3, batchSize = 5, device =\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536, 300)\n"
     ]
    }
   ],
   "source": [
    "print((trainData).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count number of unique words in a dataset\n",
    "def getWordCount(data, textVariable = 'text'):\n",
    "    vectorizer = CountVectorizer(stop_words=[], tokenizer= nltk.word_tokenize)\n",
    "    vectorizer.fit(data[textVariable])\n",
    "    cvFit = vectorizer.transform(data[textVariable])\n",
    "    wordList = list(vectorizer.vocabulary_.keys())\n",
    "    counts = np.asarray(cvFit.sum(axis=0))[0]\n",
    "\n",
    "    wordCount = []\n",
    "    for i in range(len(wordList)):\n",
    "        wordCount.append(counts[vectorizer.vocabulary_[wordList[i]]])\n",
    "    df = pd.DataFrame({'word':wordList,'wordCount':wordCount})    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_train_df = getWordCount(trainDf)\n",
    "count_test_df = getWordCount(testDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9081\n",
      "2890\n"
     ]
    }
   ],
   "source": [
    "print(len(count_train_df))\n",
    "print(len(count_test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"/data2/link10/models/fasttext/en_fasttext_crawl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a count of words in the dataset, tensor representation for dataset, \n",
    "# word frequency for the words, and a mapping from a word to the the review IDs in which the \n",
    "# word is present\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer1 = Tokenizer(nlp1.vocab)\n",
    "\n",
    "def getWordVectors(dataframe, data_name = \"text\"):\n",
    "    init_cnt=0\n",
    "    freq_dict = {}\n",
    "    occur_dict ={}\n",
    "    review_idx=-1\n",
    "    with nlp1.disable_pipes():\n",
    "        for msg in tqdm(dataframe[data_name]): \n",
    "            review_idx+=1\n",
    "            for word in tokenizer1(msg.lower()):\n",
    "        \n",
    "                if(str(word)) not in occur_dict:\n",
    "                    occur_dict[str(word)]=list()\n",
    "                occur_dict[str(word)].append(review_idx)\n",
    "                \n",
    "                \n",
    "                if str(word) in freq_dict:\n",
    "                    freq_dict[str(word)] += 1\n",
    "                else:\n",
    "                    freq_dict.update({str(word): 1}) \n",
    "                    \n",
    "                init_cnt+=1\n",
    "                \n",
    "        assert(review_idx==len(dataframe)-1)\n",
    "            \n",
    "        print(init_cnt);\n",
    "        bg = torch.empty((init_cnt,300))\n",
    "        curr_cnt=0\n",
    "        for msg in tqdm(dataframe[data_name]):\n",
    "            for word in tokenizer1(msg.lower()):\n",
    "                word_vec = nlp1(str(word)).vector\n",
    "                curr_embed = torch.from_numpy(word_vec)\n",
    "                curr_embed = curr_embed.reshape((1, len(curr_embed)))\n",
    "                bg[curr_cnt] = curr_embed\n",
    "                curr_cnt+=1\n",
    "                \n",
    "        return init_cnt, bg, freq_dict, occur_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNNet(\n",
      "  (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "(1536, 300)\n"
     ]
    }
   ],
   "source": [
    "print(NNnet)\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData = trainData[:10]\n",
    "train_bg = torch.FloatTensor(trainData)\n",
    "train_bg = train_bg.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = NNnet(train_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = op.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 300])\n"
     ]
    }
   ],
   "source": [
    "print(train_bg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bg =  train_bg[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Explainer object with the training data as background\n",
    "e = shap.DeepExplainer(NNnet, train_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.63814449310303\n"
     ]
    }
   ],
   "source": [
    "# Calculating SHAP values\n",
    "st = time.time()\n",
    "shap_vals = e.shap_values(train_bg)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00496835  0.00279992  0.00386436 ... -0.00370257 -0.0010992\n",
      "   0.00328766]\n",
      " [-0.00110735 -0.00419038  0.01285128 ... -0.00277276 -0.00399743\n",
      "   0.00316105]\n",
      " [-0.00679045  0.00054951  0.01143065 ... -0.00370035  0.00020071\n",
      "  -0.00886614]\n",
      " ...\n",
      " [-0.00318277  0.00424367  0.00521297 ...  0.00466901  0.00224051\n",
      "  -0.00228566]\n",
      " [ 0.00036497  0.0008125   0.00757837 ...  0.00558576 -0.02169074\n",
      "  -0.00109567]\n",
      " [ 0.00973777  0.00620956  0.00214267 ... -0.0011055  -0.00089916\n",
      "  -0.00048995]]\n",
      "[[ 4.48817480e-03 -2.54443381e-03 -3.44792823e-03 ...  3.48240277e-03\n",
      "   1.18693174e-03 -2.91970908e-03]\n",
      " [ 9.89170629e-04  3.85089614e-03 -1.16054825e-02 ...  2.61257985e-03\n",
      "   3.81547352e-03 -2.80752173e-03]\n",
      " [ 6.13943674e-03 -4.85563127e-04 -1.03159640e-02 ...  3.48032941e-03\n",
      "   7.98672863e-06  7.85006024e-03]\n",
      " ...\n",
      " [ 2.87000416e-03 -3.86530347e-03 -4.67207795e-03 ... -4.34910879e-03\n",
      "  -1.84200518e-03  2.01894064e-03]\n",
      " [-3.45104141e-04 -7.26166589e-04 -6.81919511e-03 ... -5.20671625e-03\n",
      "   1.98623799e-02  9.64461011e-04]\n",
      " [-8.83913599e-03 -5.66387037e-03 -1.88512285e-03 ...  1.05287496e-03\n",
      "   1.00551185e-03  4.27718740e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(shap_vals[0])\n",
    "print(shap_vals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "7O1khsu9b4Nd",
    "outputId": "d0119827-64de-481e-e5a7-6ad86ab09f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 300)\n"
     ]
    }
   ],
   "source": [
    "print((shap_vals[1].shape))\n",
    "shap_scores = shap_vals[1].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "tE0UZdh5__8l",
    "outputId": "5c768f7c-a149-4d59-f4d1-cd7df79ce7ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [00:01<00:00, 1044.61it/s]\n",
      "  1%|          | 14/1536 [00:00<00:11, 131.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [00:10<00:00, 148.20it/s]\n"
     ]
    }
   ],
   "source": [
    "explain_cnt,explain_examples, explain_dict, explain_occur = getWordVectors(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([127310, 300])\n"
     ]
    }
   ],
   "source": [
    "print(explain_examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13274\n"
     ]
    }
   ],
   "source": [
    "print(len(explain_occur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuLbh7GHJ4f9"
   },
   "outputs": [],
   "source": [
    "# Function which finds the shap score for each word/token\n",
    "# by averaging the shap scores of all the reviews in which the word occurs\n",
    "\n",
    "def findShapForToken(shap_scores, explain_occur, explain_dict):\n",
    "    explain_scores={}\n",
    "    for word in explain_occur:\n",
    "        indices = explain_occur[word]\n",
    "        word_score = 0\n",
    "        assert(explain_dict[word]==len(indices))\n",
    "        for index in indices:\n",
    "            if(index>=len(explain_occur)):\n",
    "                print(word)\n",
    "            word_score += shap_scores[index]\n",
    "        explain_scores[word] = word_score/len(indices)\n",
    "    return explain_scores       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X38r6yCXc3WW"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 514 is out of bounds for axis 0 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-71c3279e0001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_scores\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfindShapForToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplain_occur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-087d885810a2>\u001b[0m in \u001b[0;36mfindShapForToken\u001b[0;34m(shap_scores, explain_occur, explain_dict)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplain_occur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mword_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mshap_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mexplain_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexplain_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 514 is out of bounds for axis 0 with size 500"
     ]
    }
   ],
   "source": [
    " word_scores =findShapForToken(shap_scores, explain_occur, explain_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRYOx4w7l5Un"
   },
   "outputs": [],
   "source": [
    "# Converting into final lexicon format\n",
    "final_data = []\n",
    "for token in word_scores:\n",
    "    curr_token_data=[]\n",
    "    curr_token_data.append(token)\n",
    "    curr_token_data.append(word_scores[token])\n",
    "    curr_token_data.append(explain_dict[token])\n",
    "    final_data.append(curr_token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =  pd.DataFrame(final_data, columns =['word', 'score', 'word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE LEXICON SAVING DESTINATION HERE\n",
    "\n",
    "final_df.to_csv(path_or_buf=\"./FINAL_LEX/ffn_deepshap_nrc_anger_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"/home/tjss/Final/embedding-lexica-creation/lexica/FFN_DeepShap/nrc_anger_ffn_deepshap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to be evaluated against\n",
    "\n",
    "dataList = ['nrc_anger', 'song_anger', 'dialog_anger', 'friends_anger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconWords, lexiconMap = getLexicon(df = final_df)\n",
    "\n",
    "results = []\n",
    "\n",
    "for data in dataList:\n",
    "    results.append(testFFN(NNnet,data,lexiconWords, lexiconMap, nlp, dataFolder))\n",
    "    \n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['TestData','modelAcc', 'modelF1', 'lexiconAcc', 'lexiconF1']\n",
    "results.to_csv(\"Results.csv\",index = False, index_label = False)\n",
    "print(\"--------------------\"+lexiconDataset+\"-----------------------------\")\n",
    "print(str(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "MeanEmbeddingsFasttext_NRCJoyVsNotJoy_wordLevelBackground(All Reviews)Shap.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
