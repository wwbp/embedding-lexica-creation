{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/Datasets/')\n",
    "\n",
    "from createDataset import *\n",
    "\n",
    "import spacy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getData in module createDataset:\n",
      "\n",
      "getData(dataFolder, dataset, balanceTrain=True)\n",
      "    Available options for dataset\n",
      "        yelp\n",
      "        yelp_subset\n",
      "        yelp_1v5\n",
      "        amazon_finefood\n",
      "        amazon_finefood_subset\n",
      "        amazon_toys\n",
      "        amazon_toys_subset\n",
      "        empathy\n",
      "        nrc_joy\n",
      "        nrc_sadness\n",
      "        nrc_fear\n",
      "        nrc_anger\n",
      "        nrc_surprise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(getData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "df = getExternalData(dataFolder, 'dialog')\n",
    "\n",
    "# trainDf, valDf, testDf = getData(dataFolder,'amazon_finefood_subset')\n",
    "# trainDf = trainDf[['text','label']]\n",
    "# trainDf = trainDf.drop_duplicates()\n",
    "# print(len(trainDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv('/home/roshansk/YelpAnalysis/Lexicons/LSTM/nyelp_subset_lstm_div.csv')\n",
    "\n",
    "lexiconWords = set(lexicon.word.values)\n",
    "\n",
    "lexiconMap = {}\n",
    "\n",
    "for i in range(len(lexicon)):\n",
    "    lexiconMap[lexicon.iloc[i]['word']] = lexicon.iloc[i]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24923"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexiconWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLexicon(file):\n",
    "    \n",
    "    lexicon = pd.read_csv(file)\n",
    "    \n",
    "    if 'scores' in lexicon.columns:\n",
    "        lexicon.rename({'scores':'score'},axis =1, inplace = True)\n",
    "\n",
    "    lexiconWords = set(lexicon.word.values)\n",
    "\n",
    "    lexiconMap = {}\n",
    "\n",
    "    for i in range(len(lexicon)):\n",
    "        lexiconMap[lexicon.iloc[i]['word']] = lexicon.iloc[i]['score']\n",
    "        \n",
    "    return lexiconWords, lexiconMap\n",
    "\n",
    "\n",
    "def scoreText(text, lexiconWords, lexiconMap):\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    tokenList = [token.text for token in doc]\n",
    "\n",
    "    for token in tokenList:\n",
    "        if token in lexiconWords:\n",
    "            score += lexiconMap[token]\n",
    "            \n",
    "    return score/len(tokenList)\n",
    " \n",
    "def evaluateLexicon(testDf, lexiconWords, lexiconMap, dataName, lexiconName):\n",
    "    \n",
    "    ### Getting lexicon scores for text\n",
    "    scoreList = []\n",
    "\n",
    "    for i in range(len(testDf)):\n",
    "        score = scoreText(testDf.iloc[i]['text'].lower(), lexiconWords, lexiconMap)\n",
    "\n",
    "        scoreList.append(score)\n",
    "        \n",
    "    testDf['score'] = scoreList\n",
    "    \n",
    "    ### Training model for classification\n",
    "    model = LogisticRegression()\n",
    "    X = testDf.score.values.reshape(-1,1)\n",
    "    y = testDf.label\n",
    "    \n",
    "    \n",
    "    ### Computing Metrics\n",
    "    acc = np.round(np.mean(cross_val_score(model, X, y, cv=5, scoring='accuracy')),3)\n",
    "    f1 = np.round(np.mean(cross_val_score(model, X, y, cv=5, scoring='f1')),3)\n",
    "    precision = np.round(np.mean(cross_val_score(model, X, y, cv=5, scoring='average_precision')),3)\n",
    "    auc = np.round(np.mean(cross_val_score(model, X, y, cv=5, scoring='roc_auc')),3)\n",
    "\n",
    "#     print(f\" ACC | F1 | Precision | AUC \")\n",
    "    print(f\" {dataName} , {lexiconName} , {acc} , {f1} , {precision} , {auc}\")\n",
    "    \n",
    "    \n",
    "def runExperiment(testDf, lexiconList,  dataName):\n",
    "    \n",
    "    \n",
    "    for lexicon in lexiconList:\n",
    "        lexiconWords, lexiconMap = getLexicon(lexicon)\n",
    "        \n",
    "        lexiconName = lexicon.replace(\"/home/roshansk/YelpAnalysis/Lexicons/\",'')\n",
    "        evaluateLexicon(trainDf, lexiconWords, lexiconMap, dataName, lexiconName)\n",
    "        \n",
    "#         print(\"-\"*10)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Dialog  | Lexicon : Yelp_subset\n",
    "evaluateLexicon(df, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_sadness_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_subset_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/empathy_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_joy_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_baby_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_1v5_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_finefood_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_surprise_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_fear_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_anger_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_toys_fasttext_results.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/FFN/NN_NRCJoy_Lexicon.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/FFN/NN_Yelp_subset_Lexicon.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_sadness_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nyelp_subset_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nyelp_1v5_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_surprise_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_finefood_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_baby_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_anger_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_toys_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/empathy_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_fear_lstm_div.csv',\n",
       " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_joy_lstm_div.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexiconList = []\n",
    "\n",
    "for path, subdirs, files in os.walk('/home/roshansk/YelpAnalysis/Lexicons/'):\n",
    "    for name in files:\n",
    "        lexiconList.append(os.path.join(path, name))\n",
    "        \n",
    "lexiconList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconList = [\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_sadness_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_subset_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/empathy_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_joy_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_baby_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/nyelp_1v5_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_finefood_fasttext_results.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_surprise_fasttext_results.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_fear_fasttext_results.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/SVM/nrc_anger_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/SVM/amazon_toys_fasttext_results.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/FFN/NN_NRCJoy_Lexicon.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/FFN/NN_Yelp_subset_Lexicon.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_sadness_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nyelp_subset_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nyelp_1v5_lstm_div.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_surprise_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_finefood_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_baby_lstm_div.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_anger_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/amazon_toys_lstm_div.csv',\n",
    " '/home/roshansk/YelpAnalysis/Lexicons/LSTM/empathy_lstm_div.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_fear_lstm_div.csv',\n",
    "#  '/home/roshansk/YelpAnalysis/Lexicons/LSTM/nrc_joy_lstm_div.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dialog , SVM/nyelp_fasttext_results.csv , 0.877 , 0.931 , 0.972 , 0.874\n",
      "----------\n",
      " dialog , SVM/nyelp_subset_fasttext_results.csv , 0.867 , 0.926 , 0.968 , 0.851\n",
      "----------\n",
      " dialog , SVM/empathy_fasttext_results.csv , 0.85 , 0.919 , 0.916 , 0.67\n",
      "----------\n",
      " dialog , SVM/nrc_joy_fasttext_results.csv , 0.853 , 0.92 , 0.933 , 0.724\n",
      "----------\n",
      " dialog , SVM/amazon_baby_fasttext_results.csv , 0.873 , 0.929 , 0.972 , 0.868\n",
      "----------\n",
      " dialog , SVM/nyelp_1v5_fasttext_results.csv , 0.874 , 0.929 , 0.972 , 0.871\n",
      "----------\n",
      " dialog , SVM/amazon_finefood_fasttext_results.csv , 0.891 , 0.938 , 0.98 , 0.904\n",
      "----------\n",
      " dialog , SVM/amazon_toys_fasttext_results.csv , 0.873 , 0.929 , 0.972 , 0.868\n",
      "----------\n",
      " dialog , FFN/NN_NRCJoy_Lexicon.csv , 0.85 , 0.919 , 0.92 , 0.676\n",
      "----------\n",
      " dialog , FFN/NN_Yelp_subset_Lexicon.csv , 0.865 , 0.925 , 0.963 , 0.835\n",
      "----------\n",
      " dialog , LSTM/nyelp_subset_lstm_div.csv , 0.851 , 0.92 , 0.938 , 0.742\n",
      "----------\n",
      " dialog , LSTM/nyelp_1v5_lstm_div.csv , 0.851 , 0.92 , 0.929 , 0.737\n",
      "----------\n",
      " dialog , LSTM/amazon_finefood_lstm_div.csv , 0.851 , 0.92 , 0.887 , 0.581\n",
      "----------\n",
      " dialog , LSTM/amazon_baby_lstm_div.csv , 0.851 , 0.92 , 0.909 , 0.626\n",
      "----------\n",
      " dialog , LSTM/amazon_toys_lstm_div.csv , 0.851 , 0.92 , 0.871 , 0.542\n",
      "----------\n",
      " dialog , LSTM/empathy_lstm_div.csv , 0.851 , 0.92 , 0.882 , 0.578\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "df = getExternalData(dataFolder, 'dialog')\n",
    "\n",
    "runExperiment(df, lexiconList, dataName='dialog')\n",
    "\n",
    "# trainDf, valDf, testDf = getData(dataFolder,'amazon_finefood_subset')\n",
    "# trainDf = trainDf[['text','label']]\n",
    "# trainDf = trainDf.drop_duplicates()\n",
    "# print(len(trainDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7688\n",
      " amazon_finefood_subset , SVM/nyelp_fasttext_results.csv , 0.877 , 0.931 , 0.972 , 0.874\n",
      " amazon_finefood_subset , SVM/nyelp_subset_fasttext_results.csv , 0.867 , 0.926 , 0.968 , 0.851\n",
      " amazon_finefood_subset , SVM/empathy_fasttext_results.csv , 0.85 , 0.919 , 0.916 , 0.67\n",
      " amazon_finefood_subset , SVM/nrc_joy_fasttext_results.csv , 0.853 , 0.92 , 0.933 , 0.724\n",
      " amazon_finefood_subset , SVM/amazon_baby_fasttext_results.csv , 0.873 , 0.929 , 0.972 , 0.868\n",
      " amazon_finefood_subset , SVM/nyelp_1v5_fasttext_results.csv , 0.874 , 0.929 , 0.972 , 0.871\n",
      " amazon_finefood_subset , SVM/amazon_finefood_fasttext_results.csv , 0.891 , 0.938 , 0.98 , 0.904\n",
      " amazon_finefood_subset , SVM/amazon_toys_fasttext_results.csv , 0.873 , 0.929 , 0.972 , 0.868\n",
      " amazon_finefood_subset , FFN/NN_NRCJoy_Lexicon.csv , 0.85 , 0.919 , 0.92 , 0.676\n",
      " amazon_finefood_subset , FFN/NN_Yelp_subset_Lexicon.csv , 0.865 , 0.925 , 0.963 , 0.835\n",
      " amazon_finefood_subset , LSTM/nyelp_subset_lstm_div.csv , 0.851 , 0.92 , 0.938 , 0.742\n",
      " amazon_finefood_subset , LSTM/nyelp_1v5_lstm_div.csv , 0.851 , 0.92 , 0.929 , 0.737\n",
      " amazon_finefood_subset , LSTM/amazon_finefood_lstm_div.csv , 0.851 , 0.92 , 0.887 , 0.581\n",
      " amazon_finefood_subset , LSTM/amazon_baby_lstm_div.csv , 0.851 , 0.92 , 0.909 , 0.626\n",
      " amazon_finefood_subset , LSTM/amazon_toys_lstm_div.csv , 0.851 , 0.92 , 0.871 , 0.542\n",
      " amazon_finefood_subset , LSTM/empathy_lstm_div.csv , 0.851 , 0.92 , 0.882 , 0.578\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "trainDf, valDf, testDf = getData(dataFolder,'amazon_finefood_subset')\n",
    "trainDf = trainDf[['text','label']]\n",
    "trainDf = trainDf.drop_duplicates()\n",
    "print(len(trainDf))\n",
    "\n",
    "runExperiment(trainDf, lexiconList, dataName = 'amazon_finefood_subset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16458\n",
      " nrc_joy , SVM/nyelp_fasttext_results.csv , 0.644 , 0.37 , 0.545 , 0.642\n",
      " nrc_joy , SVM/nyelp_subset_fasttext_results.csv , 0.64 , 0.327 , 0.529 , 0.623\n",
      " nrc_joy , SVM/empathy_fasttext_results.csv , 0.619 , 0.193 , 0.491 , 0.594\n",
      " nrc_joy , SVM/nrc_joy_fasttext_results.csv , 0.776 , 0.694 , 0.779 , 0.838\n",
      " nrc_joy , SVM/amazon_baby_fasttext_results.csv , 0.642 , 0.369 , 0.54 , 0.644\n",
      " nrc_joy , SVM/nyelp_1v5_fasttext_results.csv , 0.645 , 0.359 , 0.542 , 0.634\n",
      " nrc_joy , SVM/amazon_finefood_fasttext_results.csv , 0.648 , 0.403 , 0.552 , 0.659\n",
      " nrc_joy , SVM/amazon_toys_fasttext_results.csv , 0.642 , 0.369 , 0.54 , 0.644\n",
      " nrc_joy , FFN/NN_NRCJoy_Lexicon.csv , 0.774 , 0.693 , 0.792 , 0.844\n",
      " nrc_joy , FFN/NN_Yelp_subset_Lexicon.csv , 0.636 , 0.31 , 0.52 , 0.608\n",
      " nrc_joy , LSTM/nyelp_subset_lstm_div.csv , 0.606 , 0.027 , 0.411 , 0.501\n",
      " nrc_joy , LSTM/nyelp_1v5_lstm_div.csv , 0.604 , 0.024 , 0.443 , 0.582\n",
      " nrc_joy , LSTM/amazon_finefood_lstm_div.csv , 0.605 , 0.015 , 0.426 , 0.525\n",
      " nrc_joy , LSTM/amazon_baby_lstm_div.csv , 0.605 , 0.0 , 0.387 , 0.492\n",
      " nrc_joy , LSTM/amazon_toys_lstm_div.csv , 0.606 , 0.046 , 0.438 , 0.547\n",
      " nrc_joy , LSTM/empathy_lstm_div.csv , 0.605 , 0.0 , 0.418 , 0.533\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "trainDf, valDf, testDf = getData(dataFolder,'nrc_joy')\n",
    "trainDf = trainDf[['text','label']]\n",
    "trainDf = trainDf.drop_duplicates()\n",
    "print(len(trainDf))\n",
    "\n",
    "runExperiment(trainDf, lexiconList, dataName = 'nrc_joy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487\n",
      " empathy , SVM/nyelp_fasttext_results.csv , 0.546 , 0.446 , 0.533 , 0.546\n",
      " empathy , SVM/nyelp_subset_fasttext_results.csv , 0.549 , 0.467 , 0.541 , 0.556\n",
      " empathy , SVM/empathy_fasttext_results.csv , 0.718 , 0.704 , 0.777 , 0.789\n",
      " empathy , SVM/nrc_joy_fasttext_results.csv , 0.553 , 0.49 , 0.562 , 0.575\n",
      " empathy , SVM/amazon_baby_fasttext_results.csv , 0.526 , 0.414 , 0.535 , 0.543\n",
      " empathy , SVM/nyelp_1v5_fasttext_results.csv , 0.551 , 0.474 , 0.548 , 0.565\n",
      " empathy , SVM/amazon_finefood_fasttext_results.csv , 0.535 , 0.441 , 0.536 , 0.549\n",
      " empathy , SVM/amazon_toys_fasttext_results.csv , 0.526 , 0.414 , 0.535 , 0.543\n",
      " empathy , FFN/NN_NRCJoy_Lexicon.csv , 0.545 , 0.455 , 0.553 , 0.549\n",
      " empathy , FFN/NN_Yelp_subset_Lexicon.csv , 0.558 , 0.474 , 0.547 , 0.565\n",
      " empathy , LSTM/nyelp_subset_lstm_div.csv , 0.514 , 0.0 , 0.514 , 0.517\n",
      " empathy , LSTM/nyelp_1v5_lstm_div.csv , 0.507 , 0.1 , 0.508 , 0.532\n",
      " empathy , LSTM/amazon_finefood_lstm_div.csv , 0.514 , 0.053 , 0.495 , 0.49\n",
      " empathy , LSTM/amazon_baby_lstm_div.csv , 0.542 , 0.5 , 0.553 , 0.56\n",
      " empathy , LSTM/amazon_toys_lstm_div.csv , 0.524 , 0.217 , 0.517 , 0.516\n",
      " empathy , LSTM/empathy_lstm_div.csv , 0.516 , 0.0 , 0.535 , 0.54\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "trainDf, valDf, testDf = getData(dataFolder,'empathy')\n",
    "trainDf = trainDf[['text','label']]\n",
    "trainDf = trainDf.drop_duplicates()\n",
    "print(len(trainDf))\n",
    "\n",
    "runExperiment(trainDf, lexiconList, dataName = 'empathy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      " amazon_toys_subset , SVM/nyelp_fasttext_results.csv , 0.939 , 0.968 , 0.989 , 0.874\n",
      " amazon_toys_subset , SVM/nyelp_subset_fasttext_results.csv , 0.941 , 0.969 , 0.987 , 0.853\n",
      " amazon_toys_subset , SVM/empathy_fasttext_results.csv , 0.938 , 0.968 , 0.966 , 0.67\n",
      " amazon_toys_subset , SVM/nrc_joy_fasttext_results.csv , 0.938 , 0.968 , 0.974 , 0.733\n",
      " amazon_toys_subset , SVM/amazon_baby_fasttext_results.csv , 0.942 , 0.97 , 0.993 , 0.911\n",
      " amazon_toys_subset , SVM/nyelp_1v5_fasttext_results.csv , 0.94 , 0.969 , 0.989 , 0.87\n",
      " amazon_toys_subset , SVM/amazon_finefood_fasttext_results.csv , 0.938 , 0.968 , 0.99 , 0.878\n",
      " amazon_toys_subset , SVM/amazon_toys_fasttext_results.csv , 0.942 , 0.97 , 0.993 , 0.911\n",
      " amazon_toys_subset , FFN/NN_NRCJoy_Lexicon.csv , 0.938 , 0.968 , 0.969 , 0.691\n",
      " amazon_toys_subset , FFN/NN_Yelp_subset_Lexicon.csv , 0.939 , 0.968 , 0.985 , 0.834\n",
      " amazon_toys_subset , LSTM/nyelp_subset_lstm_div.csv , 0.938 , 0.968 , 0.971 , 0.694\n",
      " amazon_toys_subset , LSTM/nyelp_1v5_lstm_div.csv , 0.938 , 0.968 , 0.971 , 0.732\n",
      " amazon_toys_subset , LSTM/amazon_finefood_lstm_div.csv , 0.938 , 0.968 , 0.948 , 0.54\n",
      " amazon_toys_subset , LSTM/amazon_baby_lstm_div.csv , 0.938 , 0.968 , 0.965 , 0.645\n",
      " amazon_toys_subset , LSTM/amazon_toys_lstm_div.csv , 0.938 , 0.968 , 0.946 , 0.531\n",
      " amazon_toys_subset , LSTM/empathy_lstm_div.csv , 0.938 , 0.968 , 0.959 , 0.623\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "trainDf, valDf, testDf = getData(dataFolder,'amazon_toys_subset')\n",
    "trainDf = trainDf[['text','label']]\n",
    "trainDf = trainDf.drop_duplicates()\n",
    "print(len(trainDf))\n",
    "\n",
    "runExperiment(trainDf, lexiconList, dataName = 'amazon_toys_subset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      " yelp , SVM/nyelp_fasttext_results.csv , 0.915 , 0.946 , 0.988 , 0.961\n",
      " yelp , SVM/nyelp_subset_fasttext_results.csv , 0.924 , 0.951 , 0.991 , 0.967\n",
      " yelp , SVM/empathy_fasttext_results.csv , 0.775 , 0.871 , 0.902 , 0.724\n",
      " yelp , SVM/nrc_joy_fasttext_results.csv , 0.791 , 0.878 , 0.917 , 0.773\n",
      " yelp , SVM/amazon_baby_fasttext_results.csv , 0.882 , 0.926 , 0.978 , 0.93\n",
      " yelp , SVM/nyelp_1v5_fasttext_results.csv , 0.908 , 0.941 , 0.988 , 0.959\n",
      " yelp , SVM/amazon_finefood_fasttext_results.csv , 0.891 , 0.931 , 0.98 , 0.936\n",
      " yelp , SVM/amazon_toys_fasttext_results.csv , 0.882 , 0.926 , 0.978 , 0.93\n",
      " yelp , FFN/NN_NRCJoy_Lexicon.csv , 0.774 , 0.87 , 0.895 , 0.709\n",
      " yelp , FFN/NN_Yelp_subset_Lexicon.csv , 0.912 , 0.944 , 0.988 , 0.96\n",
      " yelp , LSTM/nyelp_subset_lstm_div.csv , 0.832 , 0.898 , 0.944 , 0.84\n",
      " yelp , LSTM/nyelp_1v5_lstm_div.csv , 0.803 , 0.887 , 0.928 , 0.82\n",
      " yelp , LSTM/amazon_finefood_lstm_div.csv , 0.778 , 0.875 , 0.856 , 0.639\n",
      " yelp , LSTM/amazon_baby_lstm_div.csv , 0.778 , 0.875 , 0.889 , 0.675\n",
      " yelp , LSTM/amazon_toys_lstm_div.csv , 0.778 , 0.875 , 0.826 , 0.586\n",
      " yelp , LSTM/empathy_lstm_div.csv , 0.778 , 0.875 , 0.835 , 0.591\n"
     ]
    }
   ],
   "source": [
    "### External Dataset\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'\n",
    "\n",
    "trainDf, valDf, testDf = getData(dataFolder,'yelp_subset')\n",
    "trainDf = trainDf[['text','label']]\n",
    "trainDf = trainDf.drop_duplicates()\n",
    "print(len(trainDf))\n",
    "\n",
    "runExperiment(trainDf, lexiconList, dataName = 'yelp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN_yelp_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13538/13538 [00:04<00:00, 2764.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.835 , 0.905 , 0.949 , 0.824\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:05<00:00, 1420.91it/s]\n",
      "/home/roshansk/Covid_Bert/env/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.864 , 0.924 , 0.963 , 0.835\n"
     ]
    }
   ],
   "source": [
    "# Test : amazon_finefood_subset  | Lexicon : Yelp_subset\n",
    "evaluateLexicon(trainDf, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:08<00:00, 992.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.939 , 0.968 , 0.985 , 0.834\n"
     ]
    }
   ],
   "source": [
    "# Test : amazon_toys_subset  | Lexicon : Yelp_subset\n",
    "evaluateLexicon(trainDf, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nyelp_subset_lstm_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13538/13538 [00:04<00:00, 2743.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.816 , 0.899 , 0.912 , 0.698\n"
     ]
    }
   ],
   "source": [
    "# Test : Dialog  | Lexicon : nyelp_subset_lstm_div\n",
    "evaluateLexicon(df, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7688/7688 [00:04<00:00, 1666.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.851 , 0.92 , 0.938 , 0.742\n"
     ]
    }
   ],
   "source": [
    "# Test : amazon_finefood_subset  | Lexicon : nyelp_subset_lstm_div\n",
    "evaluateLexicon(trainDf, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:07<00:00, 1000.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACC | F1 | Precision | AUC \n",
      " 0.938 , 0.968 , 0.971 , 0.694\n"
     ]
    }
   ],
   "source": [
    "# Test : amazon_toys_subset  | Lexicon : nyelp_subset_lstm_div\n",
    "evaluateLexicon(trainDf, lexiconWords, lexiconMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22076.000000\n",
       "mean         3.641149\n",
       "std          2.357135\n",
       "min         -6.787894\n",
       "25%          2.089142\n",
       "50%          3.264231\n",
       "75%          4.840550\n",
       "max         25.217315\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scoreList).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set the list of thresholds here\n",
    "\n",
    "thresholdList = [-20,-10,0,3,4,5, 10,20,30]\n",
    "\n",
    "# thresholdList = [-.3,-.2,-.1,0,.1,.2,.3,]\n",
    "\n",
    "# thresholdList = [-20,-10,-.3,-.2,-.1,0,.1,.2,.3,10,20,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold :  -20 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n",
      "Threshold :  -10 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n",
      "Threshold :    0 Acc : 0.517  F1 : 0.517 Prec : 0.517  Recall : 0.517\n",
      "Threshold :    3 Acc : 0.709  F1 : 0.709 Prec : 0.709  Recall : 0.709\n",
      "Threshold :    4 Acc : 0.708  F1 : 0.708 Prec : 0.708  Recall : 0.708\n",
      "Threshold :    5 Acc : 0.663  F1 : 0.663 Prec : 0.663  Recall : 0.663\n",
      "Threshold :   10 Acc : 0.514  F1 : 0.514 Prec : 0.514  Recall : 0.514\n",
      "Threshold :   20 Acc : 0.501  F1 : 0.501 Prec : 0.501  Recall : 0.501\n",
      "Threshold :   30 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n"
     ]
    }
   ],
   "source": [
    "for threshold in thresholdList:\n",
    "    \n",
    "    df['pred'] = df.score.apply(lambda x : 1 if x> threshold else 0)\n",
    "    \n",
    "    acc = np.round(accuracy_score(df.label, df.pred),3)\n",
    "    f1 = np.round(f1_score(df.label, df.pred, average = 'micro'),3)\n",
    "    recall = np.round(recall_score(df.label, df.pred, average = 'micro'),3)\n",
    "    precision = np.round(precision_score(df.label, df.pred, average = 'micro'),3)\n",
    "    \n",
    "    print(f\"Threshold : {threshold:4} Acc : {acc:5}  F1 : {f1:5} Prec : {precision:5}  Recall : {recall:5}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold :  -20 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n",
      "Threshold :  -10 Acc : 0.501  F1 : 0.501 Prec : 0.501  Recall : 0.501\n",
      "Threshold :    0 Acc : 0.769  F1 : 0.769 Prec : 0.769  Recall : 0.769\n",
      "Threshold :   10 Acc : 0.503  F1 : 0.503 Prec : 0.503  Recall : 0.503\n",
      "Threshold :   20 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n",
      "Threshold :   30 Acc :   0.5  F1 :   0.5 Prec :   0.5  Recall :   0.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/roshansk/YelpAnalysis/ExternalData_Evaluation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    11038\n",
       "0     2500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = df.score.apply(lambda x : 1 if x> threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's a good idea . I hear Mary and Sally oft...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.046380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sounds great to me ! If they are willing , we ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good.Let ' s go now .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All right .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I ’ m looking at my horoscope for this month !...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label     score  pred\n",
       "0  That's a good idea . I hear Mary and Sally oft...      1 -0.046380     0\n",
       "1  Sounds great to me ! If they are willing , we ...      1  0.000971     0\n",
       "2                              Good.Let ' s go now .      1  0.088895     0\n",
       "3                                        All right .      1  0.016254     0\n",
       "7  I ’ m looking at my horoscope for this month !...      1  0.153852     0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22076.000000\n",
       "mean         3.641149\n",
       "std          2.357135\n",
       "min         -6.787894\n",
       "25%          2.089142\n",
       "50%          3.264231\n",
       "75%          4.840550\n",
       "max         25.217315\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function precision_score in module sklearn.metrics._classification:\n",
      "\n",
      "precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "    Compute the precision\n",
      "    \n",
      "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "    true positives and ``fp`` the number of false positives. The precision is\n",
      "    intuitively the ability of the classifier not to label as positive a sample\n",
      "    that is negative.\n",
      "    \n",
      "    The best value is 1 and the worst value is 0.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : list, optional\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "    \n",
      "        .. versionchanged:: 0.17\n",
      "           parameter *labels* improved for multiclass problem.\n",
      "    \n",
      "    pos_label : str or int, 1 by default\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "    \n",
      "    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "    \n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        Precision of the positive class in binary classification or weighted\n",
      "        average of the precision of each class for the multiclass task.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import precision_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> precision_score(y_true, y_pred, average='macro')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average='micro')\n",
      "    0.33...\n",
      "    >>> precision_score(y_true, y_pred, average='weighted')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.66..., 0.        , 0.        ])\n",
      "    >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.33..., 0.        , 0.        ])\n",
      "    >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "    array([0.33..., 1.        , 1.        ])\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false positive == 0``, precision returns 0 and\n",
      "    raises ``UndefinedMetricWarning``. This behavior can be\n",
      "    modified with ``zero_division``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
