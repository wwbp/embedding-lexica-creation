{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, paired_euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import fasttext\n",
    "import sister\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/')\n",
    "from utils import *\n",
    "\n",
    "import spacy\n",
    "\n",
    "# ft = fasttext.load_model('/home/roshansk/YelpAnalysis/RandomForest/cc.en.300.bin')\n",
    "\n",
    "# embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')\n",
    "# nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl_subword/')\n",
    "\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/Datasets/')\n",
    "from createDataset import *\n",
    "\n",
    "dataFolder = '/home/roshansk/YelpAnalysis/Datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFFN(trainData, testData, num_epochs = 5, batchSize = 5, device='cuda:0'):\n",
    "    \n",
    "        \n",
    "    NNnet = NNNet()    \n",
    "    NNnet.to(device)\n",
    "\n",
    "    classLoss = nn.CrossEntropyLoss()\n",
    "    reconLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    optimizer_NN = torch.optim.Adam(filter(lambda p: p.requires_grad, NNnet.parameters()), lr=0.0001)\n",
    "\n",
    "    \n",
    "\n",
    "    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=batchSize, \n",
    "                                              shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "    testLoader = torch.utils.data.DataLoader(testDataset, batch_size=batchSize, \n",
    "                                              shuffle=False, num_workers=1)\n",
    "    \n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_epoch_NN(NNnet, trainLoader, testLoader, optimizer_NN)\n",
    "        \n",
    "        \n",
    "    return NNnet\n",
    "    \n",
    "    \n",
    "def generateLexicon(NNnet, trainDf, device = 'cuda:0'):\n",
    "    \n",
    "    wordPred = getWordPred(NNnet, trainDf, nlp, device)\n",
    "    \n",
    "    return wordPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf, devDf, testDf = getData(dataFolder, 'nrc_joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "\n",
    "testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "\n",
    "trainDataset = Dataset(trainDf, trainData)\n",
    "testDataset = Dataset(testDf, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "421it [00:01, 321.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.7448931116389549 F1 : 0.7060755336617406\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "421it [00:01, 234.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.7700712589073634 F1 : 0.7070217917675544\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "421it [00:01, 214.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.7605700712589074 F1 : 0.6589986468200271\n",
      "*************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NNnet = trainFFN(trainData, testData, num_epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = generateLexicon(NNnet,trainDf, device ='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector = nlp('bookmark').vector\n",
    "# NNnet(torch.from_numpy(vector.reshape(1,-1)).to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.rename({'NNprob':'score'},axis = 1, inplace = True)\n",
    "lexiconWords, lexiconMap = getLexicon(0, df = lexicon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ,  , 0.866 , 0.913 , 0.973 , 0.923\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFFN_direct( NNnet, dataset, lexiconWords, lexiconMap, nlp, train = False):\n",
    "    \n",
    "    if dataset != 'dialog':\n",
    "        trainDf, devDf, testDf = getData(dataFolder, dataset)\n",
    "    else:\n",
    "        testDf = getExternalData(dataFolder, 'dialog')\n",
    "\n",
    "        \n",
    "    if train:\n",
    "        testData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "        testDataset = Dataset(trainDf, testData)\n",
    "        testDf = trainDf\n",
    "    else:\n",
    "        testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "        testDataset = Dataset(testDf, testData)\n",
    "        \n",
    "    testLoader = torch.utils.data.DataLoader(testDataset, batch_size=5, \n",
    "                                              shuffle=False, num_workers=1)\n",
    "    \n",
    "    \n",
    "    modelAcc, modelF1 = testModel_NN(NNnet, testLoader)\n",
    "    \n",
    "#     print(f\"Acc : {acc} F1 : {f1}\" )\n",
    "    \n",
    "    \n",
    "    lexAcc, lexF1 = evaluateLexicon(testDf, lexiconWords, lexiconMap, nlp, returnScores = True)\n",
    "    \n",
    "    print(f\"{dataset} , {modelAcc} , {modelF1} , {lexAcc} , {lexF1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2708it [00:08, 306.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog , 0.7348205052444969 , 0.8156137647663072 , 0.828 , 0.901\n"
     ]
    }
   ],
   "source": [
    "testFFN_direct(NNnet,'dialog',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFFN_direct(NNnet,'dialog',lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2708it [00:07, 363.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog , 0.4027182744866302 , 0.4372216035634744 , 0.815 , 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "200it [00:00, 222.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_finefood_subset , 0.308 , 0.32289628180039137 , 0.85 , 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "200it [00:00, 244.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_toys_subset , 0.157 , 0.1886429258902791 , 0.94 , 0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "200it [00:00, 261.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_subset , 0.385 , 0.3263964950711939 , 0.758 , 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "38it [00:00, 134.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empathy , 0.489247311827957 , 0.0 , 0.505 , 0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/roshansk/YelpAnalysis/utils.py:530: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "421it [00:02, 206.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrc_joy , 0.7605700712589074 , 0.6589986468200271 , 0.724 , 0.635\n"
     ]
    }
   ],
   "source": [
    "dataList = ['dialog','amazon_finefood_subset','amazon_toys_subset','yelp_subset','empathy','nrc_joy']\n",
    "for data in dataList:\n",
    "    testFFN_direct(NNnet,data,lexiconWords, lexiconMap, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
