{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity, paired_euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import fasttext\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/roshansk/YelpAnalysis/')\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl')\n",
    "# nlp = spacy.load('/data2/link10/models/fasttext/en_fasttext_crawl_subword/')\n",
    "\n",
    "sys.path.insert(0,'/data2/Datasets/')\n",
    "from preprocess import *\n",
    "\n",
    "dataFolder = '/data2/Datasets/Raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf, devDf, testDf = splitData(getData(dataFolder, 'empathy'))\n",
    "\n",
    "trainData = generateFastTextData_Spacy(trainDf, nlp, textVariable = 'text')\n",
    "\n",
    "testData = generateFastTextData_Spacy(testDf, nlp, textVariable = 'text')\n",
    "\n",
    "model = trainSVM(trainData, testData, trainDf, testDf)\n",
    "\n",
    "lexicon = generateLexicon_SVM(model,trainDf, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>score</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>SVM_Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>circuses</td>\n",
       "      <td>41.932019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7783</th>\n",
       "      <td>graham</td>\n",
       "      <td>32.510310</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>fernandez</td>\n",
       "      <td>32.277996</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>shipwreck</td>\n",
       "      <td>29.755081</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>job.i</td>\n",
       "      <td>26.816461</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word      score  wordCount  SVM_Rank\n",
       "5346   circuses  41.932019          1         0\n",
       "7783     graham  32.510310          1         1\n",
       "4070  fernandez  32.277996          2         2\n",
       "3278  shipwreck  29.755081          2         3\n",
       "7312      job.i  26.816461          1         4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconWords, lexiconMap = getLexicon(df = lexicon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialog , 0.838 , 0.908 , 0.838 , 0.907\n",
      "amazon_finefood_subset , 0.869 , 0.928 , 0.872 , 0.929\n",
      "amazon_toys_subset , 0.94 , 0.969 , 0.935 , 0.966\n",
      "yelp_subset , 0.884 , 0.924 , 0.879 , 0.921\n",
      "empathy , 0.505 , 0.614 , 0.548 , 0.621\n",
      "nrc_joy , 0.635 , 0.366 , 0.629 , 0.333\n"
     ]
    }
   ],
   "source": [
    "dataList = ['dialog','amazon_finefood_subset','amazon_toys_subset','yelp_subset','empathy','nrc_joy']\n",
    "for data in dataList:\n",
    "    results = testSVM(model, data,lexiconWords, lexiconMap, nlp, dataFolder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "songs , 0.7744322344322344 , 0.8711197856579798 , 0.777 , 0.874\n",
      "emobank , 0.37616689332460285 , 0.5391576140313964 , 0.626 , 0.013\n"
     ]
    }
   ],
   "source": [
    "dataList = ['songs','emobank']\n",
    "for data in dataList:\n",
    "    results = testSVM(model, data,lexiconWords, lexiconMap, nlp, dataFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
